{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento de linguagem natural (PLN) - Láb 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U langchain sentence_transformers pdfplumber spacy nltk chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pt-core-news-sm\n",
      "Successfully installed pt-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain-chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLN com Embedding em VectorDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas para PLN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunks e embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gustavoalves/Documents/GitHub/ia_PLN/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura de PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Rodar apenas uma vez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gustavoalves/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotecas para Banco de Dados Vetorial (Vector Database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando PLN - Preparação de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando PDFs\n",
    "\n",
    "def read_pdf(pdf_path):\n",
    "    pdf_reader = pdfplumber.open(pdf_path)\n",
    "\n",
    "    page = pdf_reader.pages[0]\n",
    "\n",
    "    text = ''\n",
    "\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "    \n",
    "    text = text.replace('\\n', \" \")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando PDFs\n",
    "\n",
    "def read_pdf2(pdf_path):\n",
    "    pdf_reader = pdfplumber.open(pdf_path)\n",
    "\n",
    "    page = pdf_reader.pages[0]\n",
    "\n",
    "    text = ''\n",
    "\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "    \n",
    "    # text = text.replace('\\n', \" \")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando PDF\n",
    "\n",
    "file_path = 'Classificação_de_objetos_celestes.pdf'\n",
    "\n",
    "pdf_text = read_pdf(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64064"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Classificação de Objetos Celestes February 24, 2025 1 Introdução As características espectrais são fundamentais para diferenciar distintos tipos de objetos celestes, como estrelas, galáxias e quasares. As estrelas emitem radiação eletromagnética de maneira semelhante a um corpo negro, exibindo linhas de absorção específicas. No caso das galáxias, o seu espectro é resultado da soma dos espectros de todas as estrelas e de demais materiais radiantes que as compõem. Os quasares por outro lado, apresentam linhas de emissão marcantes, frequentemente acompanhadas por um desvio expressivo para o vermelho (redshift). 2 Motivação e Contexto do DataSet OSloan Digital Sky Survey (SDSS)éumprojetodepesquisaastronômicabaseadoemimagens, conduzido com um telescópio grande angular de 2,5 metros localizado no Observatório de Apache Point, no Novo México (Estados Unidos). O objetivo do projeto é mapear um quarto do céu visível, obter observações de aproximadamente 100 milhões de objetos e o espectro de um milhão de obje- tos. O levantamento inclui informações espectrais e fotométricas de todos os objetos astronômicos detectados, incluindo estrelas, galáxias e quasares. Os dados são publicados periodicamente e disponibilizados publicamente. Este conjunto de dados corresponde aos dados da Data Release 17 (DR17) [1]. . 2.1 Importância do Estudo Categorizar objetos astronômicos com base em características espectrais e fotométricas é essencial para entender a evolução estelar e a formação de galáxias. As análises espectrais revelam a com- posição e temperatura das estrelas, enquanto as características fotométricas fornecem dados sobre luminosidade e distância. Juntas, essas informações ajudam a mapear a história do universo e os processos que moldam sua estrutura. 3 Dados • obj_ID: Identificador do objeto, um valor único que identifica o objeto no catálogo de ima- gens utilizado pelo arquivo de dados do Sloan Digital Sky Survey (SDSS). • alpha: Ângulo de Ascensão Reta (em graus) na época J2000, um sistema de coordenadas amplamente utilizado em astronomia para descrever a posição de objetos celestes no céu. • delta: Ângulo de Declinação (em graus) na época J2000, outro sistema de coordenadas comumente usado em astronomia para localizar objetos celestes. 1• u, g, r, i, z: Filtros fotométricos usados no sistema SDSS para medir a quantidade de luz emitida pelos objetos em diferentes faixas de comprimento de onda. Cada filtro corresponde a uma cor específica da luz: u no ultravioleta, g no verde, r no vermelho, i no infravermelho próximo e z no infravermelho. • run_ID: Número de execução que identifica uma varredura específica do céu realizada pelo SDSS. Cada varredura cobre uma região determinada do céu e recebe um número único. • rerun_ID: Número de reprocessamento que indica como a imagem foi processada, incluindo a versão do software ou o método de calibração utilizado na criação da imagem. • cam_col: Coluna da câmera, usada para identificar a linha de varredura dentro da execução. Cada varredura é dividida em múltiplas colunas da câmera para cobrir uma área maior do céu. • field_ID: Número do campo, utilizado para identificar cada campo dentro da varredura. O campo representa uma região menor dentro da coluna da câmera. • spec_obj_ID: Identificador único para objetos espectroscópicos ópticos. Isso significa que duas observações diferentes com o mesmo spec_obj_ID devem pertencer à mesma classe de objeto, que pode ser galáxia, estrela ou quasar. • class: Classe do objeto, a classificação atribuída ao objeto com base em suas propriedades espectrais. Pode ser uma galáxia, uma estrela ou um quasar. • redshift: Valor do desvio para o vermelho, baseado no aumento do comprimento de onda da luz emitida pelo objeto devido ao seu movimento em relação ao observador. Esse valor mede a expansão do universo desde o momento em que a luz foi emitida pelo objeto. • plate: Identificador da placa utilizada na pesquisa espectroscópica do SDSS. Cada placa contém múltiplas fibras que coletam a luz de diferentes objetos. • MJD: Data Juliana Modificada, utilizada para indicar quando um determinado conjunto de dados do SDSS foi coletado. É uma versão modificada da Data Juliana, um sistema padronizado para representar datas e horários em astronomia. • fiber_ID: Identificador da fibra óptica que direcionou a luz para o plano focal durante cada observação. Cada fibra coleta a luz de um objeto diferente, permitindo que o SDSS observe vários objetos simultaneamente. Mais informações sobre os atributos deste DataSet podem ser encontradas aqui [2]. 4 Atributo alvo: Classe 4.1 Classes do Problema: Estrelas, Galáxias e Quasares 4.1.1 Estrelas As estrelas são enormes esferas de gás quente, compostas principalmente de hidrogênio e hélio. Elas se formam a partir do colapso de nuvens moleculares frias e densas, onde a gravidade reúne matéria até criar uma protoestrela, que eventualmente inicia a fusão nuclear. Duranteafaseprincipaldavidadeumaestrela,afusãodohidrogênioemhéliomantémseuequilíbrio contra a gravidade. Estrelas de baixa massa brilham por trilhões de anos, enquanto as massivas 2consomem seu combustível rapidamente e vivem apenas milhões de anos. No final da vida, o esgotamento do hidrogênio faz a estrela se expandir. Estrelas de baixa massa se tornam gigantes vermelhas e terminam como anãs brancas, cercadas por nebulosas planetárias. Já estrelas massivas sofrem fusão até formar ferro, colapsam e explodem em supernovas, deixando para trás estrelas de nêutrons ou buracos negros. O material expelido por essas explosões enriquece futuras gerações de estrelas. [3] 4.1.2 Galáxias As galáxias são estruturas gigantescas compostas por estrelas, planetas e vastas nuvens de gás e poeira, todos mantidos juntos pela gravidade. Elas variam em tamanho, desde pequenas, com algumas milhares de estrelas, até gigantes que podem conter trilhões de estrelas e medir mais de um milhão de anos-luz. A maioria das grandes galáxias abriga buracos negros supermassivos em seus centros. Elasapresentamdiferentesformas,sendoasmaiscomunsasespiraiseelípticas,alémdasirregulares, que possuem aparências menos organizadas. A maioria das galáxias tem entre 10 bilhões e 13,6 bilhões de anos, com algumas sendo quase tão antigas quanto o próprio universo. A galáxia mais jovem conhecida formou-se há cerca de 500 milhões de anos. As galáxias podem se organizar em grupos de até 100 membros, mantidos unidos pela gravidade. Estruturas maiores, chamadas aglomerados, podem conter milhares de galáxias. Esses, por sua vez, podem formar superaglomerados, que não são gravitacionalmente ligados. No conjunto, supera- glomerados, vazios cósmicos e grandes estruturas formam a teia cósmica do universo. [4] 4.1.3 Quasares Os quasares são os núcleos extremamente brilhantes de galáxias ativas, alimentados por buracos negros supermassivos que consomem grandes quantidades de matéria. A matéria forma um disco de acreção ao redor do buraco negro, onde a fricção aquece o gás a milhões de graus, emitindo intensa radiação. Parte do material é ejetada em jatos colimados por campos magnéticos. No universo primitivo, fluxos de gás cósmico alimentavam os buracos negros centrais. Mais tarde, colisões entre galáxias direcionaram gás para esses buracos negros, ativando os quasares. Quasares podem brilhar até 100.000 vezes mais que a Via Láctea. Apesar disso, seu disco de acreção tem apenas algumas centenas a milhares de unidades astronômicas, comparado aos 100.000 anos-luz da Via Láctea. [5] 5 Atributos Físicos 5.1 Filtros Fotométricos Os filtros fotométricos do Sloan Digital Sky Survey (SDSS) são elementos ópticos que selecionam faixas específicas do espectro eletromagnético, permitindo a observação de objetos celestes em difer- entes comprimentos de onda. Eles são fundamentais para caracterizar a composição, temperatura e idade de estrelas, galáxias e outros corpos celestes. Eles são colocados na frente dos detectores CCD da câmera do SDSS, transmitindo apenas a luz em uma faixa específica de comprimentos de onda enquanto bloqueiam outras frequências. A eficiência 3de transmissão depende do material e do revestimento dos filtros, sendo calibrada para garantir medições consistentes e comparáveis entre diferentes observações. [6][7] Cada filtro captura características físicas específicas dos objetos astronômicos: 5.1.1 Filtro u (ultravioleta próximo, ~355 nm) Detecta estrelas jovens e quentes, pois elas emitem mais fortemente no UV. Útil para estudar a formação estelar e a presença de gás ionizado em galáxias. 5.1.2 Filtro g (verde-azulado, ~469 nm) Sensível à emissão de estrelas intermediárias e contém a linha de absorção do cálcio (Ca II H&K). Importante para classificar estrelas segundo temperatura e idade. 5.1.3 Filtro r (vermelho, ~617 nm) Inclui a linha de emissão do hidrogênio ionizado (Hα, 656.3 nm), um traçador da formação estelar em galáxias. Muito usado para diferenciar populações estelares jovens e antigas. 5.1.4 Filtro i (infravermelho próximo, ~748 nm) Capta estrelas frias e regiões ricas em poeira interestelar. Essencial para estudar a estrutura de galáxias espirais e populações estelares evoluídas. 5.1.5 Filtro z (infravermelho, ~893 nm) Permite a detecção de objetos a altos redshifts, pois a luz visível de galáxias distantes é deslocada paraoinfravermelho. Útilparaestudaraevoluçãocósmicadasgaláxias. Essesfiltrossãoprojetados para cobrir diferentes partes do espectro visível e infravermelho próximo, otimizando a fotometria em larga escala para estudos astrofísicos. 5.2 Redshift Oredshiftéumparâmetrofundamentalnaastronomiaecosmologia, quemedeodeslocamentodas linhas espectrais de um objeto para comprimentos de onda maiores devido à expansão do Universo. Ele é definido como λ −λ z = observada emitida (1) λ emitida onde λ é o comprimento de onda original da luz e λ é o comprimento de onda medido emitida observada no espectro do objeto. O redshift está diretamente relacionado à distância dos objetos devido à Lei de Hubble, que estabelece que a velocidade de recessão de uma galáxia é proporcional à sua distância v = H d (2) 0 onde H é a constante de Hubble, v a velocidade de recessão do objeto e d sua distância. v e z se 0 relacionam pela Fórmula do Efeito Doppler Relativístico 4(cid:115) 1+v/c 1+z = (3) 1−v/c tal que c é a velocidade da luz no vácuo. Portanto, quanto maior o redshift, mais distante e mais antiga é a luz que observamos do objeto, permitindo estudar diferentes épocas da evolução do Universo. [8] 5.3 Coordenadas Alpha e Delta Representam as coordenadas equatoriais dos objetos observados. Eles são usados para indicar a posição dos objetos no céu, assim como as coordenadas de latitude e longitude em mapas terrestres. A ascenssão reta, RA (right ascension) ou α, é a coordenada equivalente à longitude em mapas terrestres, mas no céu. Ela mede a posição do objeto ao longo da linha do equador celeste variando de 0° a 360°. Adeclinação, Decouδ éacoordenadaequivalenteàlatitudenosistemadecoordenadasterrestres. Ela indica a posição do objeto ao longo do eixo perpendicular à linha do equador celeste, ou seja, a distância do objeto em relação ao equador celestial variando de +90° a -90°. [9] 6 Objetivo do Estudo O objetivo deste projeto é a classificação de objetos celestes em estrelas (STAR), galáxias (GALAXY) ou quasares (QSO), utilizando diferentes modelos de classificação supervisionados e métricas de avaliação. 7 Importação de Bibliotecas e Pacotes [1]: # Versão do Python utilizada !python --version Python 3.10.9 [2]: # Importa dependências import pandas as pd import numpy as np import sklearn import time import astropy.units as u import matplotlib.pyplot as plt plt.rcParams[\\'font.family\\'] = \\'serif\\' plt.rcParams[\\'axes.labelsize\\'] = 12 import seaborn as sns sns.set_style(\"darkgrid\") sns.set_palette([\"#FF5733\", \"#33FF57\", \"#3357FF\"]) import warnings warnings.filterwarnings(\"ignore\") 5# Importa classes e funções específicas from xgboost import XGBClassifier from sklearn.ensemble import ( RandomForestClassifier, GradientBoostingClassifier ) from lightgbm import LGBMClassifier from catboost import CatBoostClassifier from sklearn.preprocessing import ( LabelEncoder, StandardScaler ) from sklearn.model_selection import ( train_test_split, RandomizedSearchCV, learning_curve, KFold, cross_validate ) from imblearn.over_sampling import SMOTE from imblearn.under_sampling import RandomUnderSampler from sklearn.metrics import ( accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_curve, auc, precision_recall_curve ) from sklearn.pipeline import Pipeline from astropy.coordinates import SkyCoord from sklearn.DataSets import make_classification from sklearn.feature_selection import mutual_info_classif from sklearn.decomposition import PCA %matplotlib inline 8 Análise Exploratória de Dados (EDA) 8.1 Carregamento dos Dados [3]: # Carrega o DataSet df = pd.read_csv(\\'star_classification.csv\\') [4]: # Visualiza as 5 primeiras linhas do DataFrame df.head() [4]: obj_ID alpha delta u g r \\\\ 0 1.237661e+18 135.689107 32.494632 23.87882 22.27530 20.39501 1 1.237665e+18 144.826101 31.274185 24.77759 22.83188 22.58444 2 1.237661e+18 142.188790 35.582444 25.26307 22.66389 20.60976 3 1.237663e+18 338.741038 -0.402828 22.13682 23.77656 21.61162 4 1.237680e+18 345.282593 21.183866 19.43718 17.58028 16.49747 6i z run_ID rerun_ID cam_col field_ID spec_obj_ID \\\\ 0 19.16573 18.79371 3606 301 2 79 6.543777e+18 1 21.16812 21.61427 4518 301 5 119 1.176014e+19 2 19.34857 18.94827 3606 301 2 120 5.152200e+18 3 20.50454 19.25010 4192 301 3 214 1.030107e+19 4 15.97711 15.54461 8102 301 3 137 6.891865e+18 class redshift plate MJD fiber_ID 0 GALAXY 0.634794 5812 56354 171 1 GALAXY 0.779136 10445 58158 427 2 GALAXY 0.644195 4576 55592 299 3 GALAXY 0.932346 9149 58039 775 4 GALAXY 0.116123 6121 56187 842 Primeiramente, podemos notar que cerca de 8 atributos do DataSet são apenas identificadores únicos, além da data da coleta, atributos fotométricos, o valor do redshift, suas coordenadas equa- toriais e a classe do objeto em questão a ser classificado. a A relevância de cada atributo deverá ser estudada, em especial os atributos de indentificação. [5]: # Analisa as dimensões do DataFrame df.shape [5]: (100000, 18) O DataSet possui 100000 amostras distribuídas entre 3 classes de objetos astronômicos, com cada objeto no catálogo sendo descrito por 17 atributos (features), além de 1 coluna que identifica sua respectiva categoria. [6]: # Destaca o tipo de cada atributo df.info() <class \\'pandas.core.frame.DataFrame\\'> RangeIndex: 100000 entries, 0 to 99999 Data columns (total 18 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 obj_ID 100000 non-null float64 1 alpha 100000 non-null float64 2 delta 100000 non-null float64 3 u 100000 non-null float64 4 g 100000 non-null float64 5 r 100000 non-null float64 6 i 100000 non-null float64 7 z 100000 non-null float64 8 run_ID 100000 non-null int64 9 rerun_ID 100000 non-null int64 10 cam_col 100000 non-null int64 11 field_ID 100000 non-null int64 12 spec_obj_ID 100000 non-null float64 13 class 100000 non-null object 14 redshift 100000 non-null float64 15 plate 100000 non-null int64 16 MJD 100000 non-null int64 717 fiber_ID 100000 non-null int64 dtypes: float64(10), int64(7), object(1) memory usage: 13.7+ MB Dentre os 17 atributos apenas o atributo class é do tipo object, sendo o restante do tipo int64 e float64. A codificação desse atributo será tratada posteriormente. [7]: # Determina a presença de valores ausentes df.isnull().sum() [7]: obj_ID 0 alpha 0 delta 0 u 0 g 0 r 0 i 0 z 0 run_ID 0 rerun_ID 0 cam_col 0 field_ID 0 spec_obj_ID 0 class 0 redshift 0 plate 0 MJD 0 fiber_ID 0 dtype: int64 Não há valores ausentes neste DataSet e portanto nenhuma técnica de preenchimento ou remoção de dados será necessária. 8.2 Análise Descritiva [8]: # Descreve estatísticas resumidas df.describe() [8]: obj_ID alpha delta u \\\\ count 1.000000e+05 100000.000000 100000.000000 100000.000000 mean 1.237665e+18 177.629117 24.135305 21.980468 std 8.438560e+12 96.502241 19.644665 31.769291 min 1.237646e+18 0.005528 -18.785328 -9999.000000 25% 1.237659e+18 127.518222 5.146771 20.352353 50% 1.237663e+18 180.900700 23.645922 22.179135 75% 1.237668e+18 233.895005 39.901550 23.687440 max 1.237681e+18 359.999810 83.000519 32.781390 g r i z \\\\ count 100000.000000 100000.000000 100000.000000 100000.000000 mean 20.531387 19.645762 19.084854 18.668810 std 31.750292 1.854760 1.757895 31.728152 min -9999.000000 9.822070 9.469903 -9999.000000 25% 18.965230 18.135828 17.732285 17.460677 50% 21.099835 20.125290 19.405145 19.004595 75% 22.123767 21.044785 20.396495 19.921120 max 31.602240 29.571860 32.141470 29.383740 8run_ID rerun_ID cam_col field_ID spec_obj_ID \\\\ count 100000.000000 100000.0 100000.000000 100000.000000 1.000000e+05 mean 4481.366060 301.0 3.511610 186.130520 5.783882e+18 std 1964.764593 0.0 1.586912 149.011073 3.324016e+18 min 109.000000 301.0 1.000000 11.000000 2.995191e+17 25% 3187.000000 301.0 2.000000 82.000000 2.844138e+18 50% 4188.000000 301.0 4.000000 146.000000 5.614883e+18 75% 5326.000000 301.0 5.000000 241.000000 8.332144e+18 max 8162.000000 301.0 6.000000 989.000000 1.412694e+19 redshift plate MJD fiber_ID count 100000.000000 100000.000000 100000.000000 100000.000000 mean 0.576661 5137.009660 55588.647500 449.312740 std 0.730707 2952.303351 1808.484233 272.498404 min -0.009971 266.000000 51608.000000 1.000000 25% 0.054517 2526.000000 54234.000000 221.000000 50% 0.424173 4987.000000 55868.500000 433.000000 75% 0.704154 7400.250000 56777.000000 645.000000 max 7.011245 12547.000000 58932.000000 1000.000000 Com base nas estatísticas resumidas, podemos tirar alguns insights: 1. A ascensão reta alpha varia de 0.0055° a 359.999°, o que faz sentido, pois cobre toda a faixa possível no céu. Também, possui média próxima de 177.6°, sugerindo que os dados podem estar mais concentrados em certas regiões. 2. Adeclinaçãodeltavaide-18.78°a83.00°,entãocobreumagrandepartedocéu,masnãocom- pletamente. Também, sua média próxima de 24.14°, o que indica mais objetos no hemisfério norte celeste. 3. Em relação aos filtros u, g, r, i, z, algumas magnitudes possuem valores anômalos (-9999.000) e podem se tratar de placeholders para dados ausentes. Devemos localizá-los imediatamente e descartá-los para que não afetem as análises posteriores. Também os filtros seguem um valor médio muito próximo. Magnitudes mínimas e máximas mostram objetos brilhantes e fracos, cobrindo uma grande faixa de brilho. 4. redshift possui um média próxima de 0.576, o que indica que a maior parte dos objetos estão relativamente distantes, com valores variando de -0.009 a 7.011. O mínimo negativo pode ser erro de medição ou objetos com movimento peculiar e o máximo de 7.011 é altíssimo e sugere a presença de quasares extremamente distantes. 5. MJD indica que as observações variam dentro de uma grande faixa de datas. 6. rerun_ID possui um desvio padrão nulo e todas outras estátisticas iguais a 301.0. Isso evidencia que todas as 100000 amostras para este atributo possui o mesmo valor e portanto não pode conter informação relevante para este estudo. Vamosverificaraconsistênciafísicadosdadoseindentificarvaloresquenãosãofisicamentepossíveis neste DataSet. [9]: # Cria dicionário com condições físicas admitidas condicoes = { \"- \\'alpha\\'\": (df[\\'alpha\\'] < 0) | (df[\\'alpha\\'] > 360), \"- \\'delta\\'\": (df[\\'delta\\'] < -90) | (df[\\'delta\\'] > 90), \"- \\'u\\'\": df[\\'u\\'] < 0, 9\"- \\'g\\'\": df[\\'g\\'] < 0, \"- \\'r\\'\": df[\\'r\\'] < 0, \"- \\'i\\'\": df[\\'i\\'] < 0, \"- \\'z\\'\": df[\\'z\\'] < 0 } # Valida a presença das condições e retorna o índice correspondente for coluna, condicao in condicoes.items(): indices_invalidos = df.index[condicao].tolist() if indices_invalidos: print( f\"{coluna} possui {len(indices_invalidos)} valores inválidos nos índices: \" f\"{\\', \\'.join(map(str, indices_invalidos))}\" ) else: print(f\"{coluna} está dentro dos limites físicos.\") - \\'alpha\\' está dentro dos limites físicos. - \\'delta\\' está dentro dos limites físicos. - \\'u\\' possui 1 valores inválidos nos índices: 79543 - \\'g\\' possui 1 valores inválidos nos índices: 79543 - \\'r\\' está dentro dos limites físicos. - \\'i\\' está dentro dos limites físicos. - \\'z\\' possui 1 valores inválidos nos índices: 79543 Como havíamos previsto, trata-se do placeholder ou erro de medição dos atributos u, g e z na linha 79543. Devemos descartar esta linha imediatamente. [10]: # Decarta valores do índice 7943 e reindexa os dados df = df.drop(index=79543) df = df.reset_index(drop=True) 8.3 Visualização dos Dados 8.3.1 Distribuição das Classes Vejamos a distribuição da contagem de amostras para cada classe. [11]: # Plot da distribuição da contagem de amostras por classe plt.figure(figsize=(10,5)) sns.countplot(x=df[\\'class\\']) plt.title(\\'Distribuição das Classes\\') [11]: Text(0.5, 1.0, \\'Distribuição das Classes\\') 10[12]: # Contagem de amostras por classe df[\\'class\\'].value_counts() [12]: GALAXY 59445 STAR 21593 QSO 18961 Name: class, dtype: int64 Podemos ver que o DataSet está desbalanceado, com cerca de três vezes mais galáxias do que estrelas e quasares. Para lidar com isso, iremos utilizar as técnicas de oversampling e undersam- pling. A técnica de oversampling envolve aleatoriamente selecionar exemplos da classe minoritária, substituir e adicionar esses exemplos ao DataSet de treino. Por outro lado, o undersampling con- siste em remover aleatoriamente exemplos da classe majoritária para equilibrar a distribuição das classes. Essas abordagens ajudam a melhorar o desempenho dos modelos ao lidar com DataSets desbalanceados. Vejamosasdistribuiçõesdosatributosemumgráficodedensidadeparacadaclassedavariável-alvo. [13]: # Cria objeto para receber o valor de amostras que a menor classe possui min_amostras = df[\\'class\\'].value_counts().min() # Cria um DataFrame balanceado agrupado por \\'class\\' df_balanceado = df.groupby(\\'class\\').sample(n=min_amostras, random_state=42) # Recebe uma lista de atributos e o número de linhas e colunas para os subplots # e retorna subplots para estimar a função densidade de probabilidade def kdeplot_balanceado(atributos, linha, col): fig, axes = plt.subplots(linha, col, figsize=(14, 8)) axes = axes.flatten() for i, atributo in enumerate(atributos): sns.kdeplot( 11data=df_balanceado, x=atributo, hue=\\'class\\', fill=False, ax=axes[i] ) axes[i].set_title(atributo) plt.tight_layout() [14]: # Cria lista para os atributos de identificação atributos_1 = [\\'MJD\\', \\'fiber_ID\\', \\'cam_col\\', \\'field_ID\\'] # Chama a função kdeplot_balanceado kdeplot_balanceado(atributos_1, 2, 2) Como podemos ver, a distribuição dos atributos cam_col e field_ID não possui comportamento suficientemente distinguível por classe, com o primeiro tendo uma distribuição uniforme e periódica e o segundo uma assimétrica, tendo os picos em comum entre as três classes. [15]: # Cria outra lista para os atributos de identificação restantes atributos_2 = [\\'plate\\', \\'obj_ID\\', \\'run_ID\\', \\'spec_obj_ID\\'] # Chama a função kdeplot_balanceado kdeplot_balanceado(atributos_2, 2, 2) 12Todos os atributos parecem possuir uma relação com a classificação das classes, em que cada distribuição se comporta de maneira característica. Nota-se também a semelhança entre as dis- tribuições dos atributos obj_ID e run_ID, bem como spec_obj_ID e plate. Pelo compor- tamento mostrado acima, parecem conter as mesmas informações um do outro. Isso será melhor evidenciado ao estudar a correlação entre eles. [16]: # Cria lista para os atributos de natureza física atributos_3 = [\\'redshift\\', \\'u\\', \\'g\\', \\'r\\', \\'i\\', \\'z\\'] # Chama a função kdeplot_balanceado kdeplot_balanceado(atributos_3, 3, 2) 13Devido aos valores altos de redshift e mais baixos de u, g e z, iremos transformá-los para a escala logarítmica para melhor visualização das suas distribuições. [17]: # Transforma as amostras do atributo \\'redshift\\' para escala logarítimica df_balanceado[\\'redshift\\'] = np.log(df[\\'redshift\\']) [18]: # Chama a função kdeplot_balanceado kdeplot_balanceado(atributos_3, 3, 2) 14Podemosnotarquetodososfiltrosfotométricospossuemdistribuiçõescaracterísticasedistinguíveis por classe. Destaque para resdhift que possui seus picos de densidade em regiões diferentes para cada classe, o que mostra que se trata de um atributo com grande potencial para a classificação de objetos estelares. [19]: # Cria lista para as coordenadas equatoriais atributos_4 = [\\'alpha\\', \\'delta\\'] # Chama a função kdeplot_balanceado kdeplot_balanceado(atributos_4, 2, 1) As coordenadas equatoriais não parecem ter comportamentos muito distinguíveis por classe, com regiões de maior e menor densidade semelhantes entre si. 8.4 Análise de Correlação 8.4.1 Correlação de Pearson A fim de determinar a necessidade de reduzir a dimensionalidade do conjunto de atributos, ire- mos reproduzir um mapa de calor de Correlação de Pearson e entender como os atributos se correlacionam linearmente entre si. Amatrizéinterpretadadaseguinteforma: -1indicaumamáxima dependência linear positiva. Isso significa que, à medida que uma variável aumenta, a outra também aumenta de forma linear. - -1 indica uma máxima dependência linear negativa. Isso significa que, à medida que uma variável aumenta, a outra diminui de forma linear. - 0 indica uma dependência linear nula. Isso significa que não há uma relação linear entre as duas variáveis. No entanto, é importante destacar 15que isso não significa que não exista qualquer tipo de relação entre as variáveis. Elas podem ter uma correlação não linear. [20]: # Matriz correlação descartando a variável-alvo e 3 identificadores pouco relevantes plt.figure(figsize=(10,5)) corr = df.drop(columns=[\\'class\\',\\'rerun_ID\\', \\'cam_col\\', \\'field_ID\\']).corr() sns.heatmap(corr, annot=True, cmap=\\'Blues\\', fmt=\".2f\", linewidths=0.1) plt.title(\\'Matriz de Correlação\\') plt.show() Como mencionado anteriormente com as curvas de distribuição de densidade, a correlação linear positiva igual a 1 entre plate e spec_obj_ID, bem como entre obj_ID e run_ID, reforça a redundância total de informação que eles possuem entre si. Também, podemos notar alta correlação linear positiva entre os filtros fotométricos, com destaque paraasbandaszei,zer,ier,ereg,assimcomoentreplateeMJD.Osatributosdecoordenadas equatoriais possuem correlação baixa, próxima a 0 com todos os outros atributos, assim como o redshift que possui correlação positiva em torno de 0,30 a 0,50 com os filtros fotométricos plate e MJD. 8.4.2 Verificação de Atributos Idênticos Vejamos se plate e spec_obj_ID, bem como obj_ID e run_ID possuem de fato valores idên- ticos. [21]: # Verifica se as amostras são identificamente igual # Retorna True se verdadeiro e False se falso df[\\'plate\\'].equals(df[\\'spec_obj_ID\\']) 16[21]: False [22]: # Verifica se as amostras são identificamente igual # Retorna True se verdadeiro e False se falso df[\\'obj_ID\\'].equals(df[\\'run_ID\\']) [22]: False Comopodemosver, elesnãopossuemvaloresidênticos. Épossívelqueumavariávelsejaumaversão transformada da outra, por exemplo, uma pode estar em logaritmo, normalizada ou padronizada. Neste caso, iremos realizar um gráfico de dispersão com regressão linear para comparar a relação entre elas. Se o ajuste linear coincidir com a dispersão dos dados, elas estão apenas escaladas e, portanto, possuem informação idêntica. [23]: # Ajuste linear com a dispersão \\'plate\\' e \\'spec_obj_ID\\' plt.figure(figsize=(10,5)) sns.regplot( data=df, x=\"plate\", y=\"spec_obj_ID\", scatter_kws={\\'alpha\\': 0.5, \\'color\\': \\'black\\'}, line_kws={\\'color\\': \\'gray\\'} ) plt.xlabel(\"Plate\") plt.ylabel(\"Spec_obj_ID\") plt.title(\"Dispersão com Regressão Linear\") plt.show() Os atributos plate e spec_obj_ID possuem informação idêntica. 17[24]: # Ajuste linear com a dispersão entre \\'run_ID` e `obj_ID\\' plt.figure(figsize=(10,5)) sns.regplot( data=df, x=\"run_ID\", y=\"obj_ID\", scatter_kws={\\'alpha\\': 0.5, \\'color\\': \\'black\\'}, line_kws={\\'color\\': \\'gray\\'} ) plt.xlabel(\"Run_ID\") plt.ylabel(\"Obj_ID\") plt.title(\"Dispersão com Regressão Linear\") plt.show() Os atributos obj_ID e run_ID também possuem informação idêntica. Vamos estudar quais as melhores decisões podem ser tomadas quanto a essa redundância, já que manter ambas as variáveis não adiciona informação útil ao modelo e apenas aumenta a dimension- alidade sem benefício. 8.4.3 Informação Mútua Para ir além e capturar tanto relações lineares quanto não lineares, iremos utilizar o método da Informação Mútua (Mutual Information - MI) que consiste em quantificar a dependência entre duas variáveis ao saber quanto o valor de uma variável reduz a incerteza sobre a outra, ou seja, ela quantifica a quantidade de informação que uma variável contém sobre outra. Essa medida pode nos fazer entender mais a relação entre os atributos e a variável-alvo, por isso será útil para o nosso estudo. 18[25]: # Calcula a Informação Mútua entre cada atributo e a variável-alvo mi = mutual_info_classif( df.drop(columns=[\\'class\\']), df[\\'class\\'], discrete_features=False ) # Crian DataFrame ordenado com os resultados mi_df = pd.DataFrame({\\'Feature\\': df.drop(columns=[\\'class\\']).columns, \\'MI\\': mi}) mi_df = mi_df.sort_values(by=\\'MI\\', ascending=False) mi_df [25]: Feature MI 13 redshift 0.801846 12 spec_obj_ID 0.304013 0 obj_ID 0.296990 14 plate 0.275763 15 MJD 0.192070 8 run_ID 0.145690 7 z 0.145515 4 g 0.120623 6 i 0.109458 3 u 0.100173 5 r 0.075523 16 fiber_ID 0.048798 2 delta 0.044376 1 alpha 0.040454 9 rerun_ID 0.010855 11 field_ID 0.006596 10 cam_col 0.002060 Como podemos ver, é notável a alta relação entre o atributo redshift e a variável-alvo. Destaque também para os atributos spec_obj_ID, obj_ID, plate e MJD. Enquanto para os atributos restantes nota-se baixa relação e relevância para a determinação da variável-alvo, destaque para os filtros fotométricos e as coordenadas equatoriais. Contudo, iremos mantê-los para análises posteri- ores. Optaremos por descartar os atributos rerun_ID, field_ID e cam_col devido à baixa relevância que eles demonstraram ter para o estudo. 8.4.4 Correlação Entre Coordenadas Equatoriais e a Variável Alvo Como foi obtido um valor baixo para a correlação das coordenadas equatoriais e a variável-alvo, vamosrealizarumgráficodedispersãodosobjetosporclasseemfunçãodeascensãoretaedeclinação eobservarasregiõesdocéuemqueasestrelas, quasaresegaláxiasforamobservadaspelotelescópio. [26]: cores = [\"#FF5733\", \"#33FF57\", \"#3357FF\"] classes = df[\\'class\\'].unique() # Configura o estilo do Seaborn sns.set(style=\\'darkgrid\\') # Cria a figura e os eixos plt.figure(figsize=(10, 5)) 19# Plot da dispersão dos objetos for i, classe in enumerate(classes): subset = df[df[\\'class\\'] == classe] plt.scatter( subset[\\'alpha\\'], subset[\\'delta\\'], color=cores[i], label=classe, alpha=0.6 # Transparência para melhor visualização ) # Adiciona rótulos e título plt.xlabel(\\'Ascensão Reta (α)\\') plt.ylabel(\\'Declinação (δ)\\') plt.title(\\'Ascensão Reta vs. Declinação\\') plt.legend(title=\\'Classe\\') plt.grid(True) # Mostra o gráfico plt.show() Pensandonaascensãoretacomooânguloquetraçaoequadorterrestre, pode-senotarapresençade duasregiõescomaltaconcentraçãodeobjetos. Também, éevidenciadoqueacoordenadageográfica onde determinado objeto da amostra foi catalogado tem pouca relevância para a sua classe. Isso é demonstrado pela região central com distribuição homogênea entre galáxias e quasares e uma presença maior de aglomerados de estrelas em certas regiões. Abaixo, uma representação da distribuição dos objetos utilizando astropy para melhor visualização 20[27]: # Cria a figura e o subplot Mollweide classe_unica = df[\\'class\\'].unique() fig = plt.figure(figsize=(15, 15)) ax = fig.add_subplot(111, projection=\\'mollweide\\') for classe_label in classe_unica: subset = df[df[\\'class\\'] == classe_label] coords = SkyCoord( ra=subset[\\'alpha\\'] * u.degree, dec=subset[\\'delta\\'] * u.degree, frame=\\'icrs\\' ) ax.scatter( coords.ra.wrap_at(180 * u.degree).radian, coords.dec.radian, s=1, label=classe_label, alpha=0.7 ) ax.grid(True) ax.set_title( \\'Mapa Celeste (Projeção Mollweide)\\', fontsize=14 ) ax.legend( loc=\\'upper left\\', fontsize=12, bbox_to_anchor=(1.05, 1), borderaxespad=0., title=\"Classe\", title_fontsize=\\'13\\', markerscale=6 ) [27]: <matplotlib.legend.Legend at 0x1f783282fb0> 21Como já observado, há uma distribuição homogênea em duas regiões principais, com certos aglom- erados de estrelas espaçadas entre essas duas regiões. Ainda que pouco evidente a relação das coordenadas equatoriais com a classificação dos objetos. 8.4.5 Correlação Entre Redshift e a Variável Alvo Podemos visualizar sua relação também com o redshift, atributo que irá dar profundidade a esses objetos no céu. [28]: plt.figure(figsize=(10, 5)) # Plot da dispersão dos objetos for i, classe in enumerate(classes): subset = df[df[\\'class\\'] == classe] plt.scatter( subset[\\'alpha\\'], subset[\\'redshift\\'], color=cores[i], label=classe, alpha=0.6 ) plt.xlabel(\\'Ascensão Reta (α)\\') plt.ylabel(\\'Redshift\\') plt.title(\\'Ascensão Reta vs. Redshift\\') plt.legend(title=\\'Classe\\') plt.grid(True) plt.show() 22[29]: plt.figure(figsize=(10, 5)) # Plot da dispersão dos objetos for i, classe in enumerate(classes): subset = df[df[\\'class\\'] == classe] plt.scatter( subset[\\'delta\\'], subset[\\'redshift\\'], color=cores[i], label=classe, alpha=0.6 ) plt.xlabel(\\'Declinação (δ)\\') plt.ylabel(\\'Redshift\\') plt.title(\\'Declinação vs. Redshift\\') plt.legend(title=\\'Classe\\') plt.grid(True) plt.show() É notável como o atributo redshift possui forte relevância para a classificação das classes. Nota- se uma distribuição bem determinada para cada classe em função da sua distância ao telescópio, com grande concentração de quasares em redshifts altos e intermediários, galáxias com redshifts intermediários e estrelas com redshifts próximos a 0. É possível notar que neste DataSet existe uma grande quantidade de estrelas próximas que são jovens e brilhantes, bem como quasares distantes que são antigos e mais ofuscados. 238.5 Identificação de Outliers Vejamos se há outliers nesta amostra. Como a escala de valores dos atributos é muito diferente entre si, realizar um boxplot pode não ser muito interessante para identificar a presença de outliers. Portanto, vamos realizar a seguinte abordagem. [30]: # Recebe uma coluna de atributos e retorna valores abaixo # do limite inferior e acima do limite superior # Utiliza 1° e 3° quartil (Q1 e Q3) e intervalo interquartil (IQR) def detecta_outliers(coluna): Q1 = coluna.quantile(0.25) Q3 = coluna.quantile(0.75) IQR = Q3 - Q1 lim_inferior = Q1 - 1.5 * IQR lim_superior = Q3 + 1.5 * IQR outliers_inferior = coluna[coluna < lim_inferior] outliers_superior = coluna[coluna > lim_superior] return outliers_inferior, outliers_superior [31]: # Dicionário para armazenar resultados de outliers dados_outlier = {} # Itera pelas colunas do DataFrame (exceto \\'class\\') for col in df.drop(columns=[\\'class\\']): inferior, superior = detecta_outliers(df[col]) # Contag total de outliers total_outliers = len(inferior) + len(superior) # Conta outliers por classe para ambos limites classe_inferior_count = df.loc[inferior.index, \\'class\\'].value_counts() classe_superior_count = df.loc[superior.index, \\'class\\'].value_counts() # Armazena dados no dicionário dados_outlier[col] = [ total_outliers, len(inferior), classe_inferior_count.get(\\'GALAXY\\', 0), classe_inferior_count.get(\\'STAR\\', 0), classe_inferior_count.get(\\'QSO\\', 0), len(superior), classe_superior_count.get(\\'GALAXY\\', 0), classe_superior_count.get(\\'STAR\\', 0), classe_superior_count.get(\\'QSO\\', 0) ] # Cria DataFrame a partir do dicionário df_outlier = pd.DataFrame.from_dict( dados_outlier, orient=\\'index\\', columns=[ \\'Outliers Total\\', \\'Lower Bound\\', \\'Galáxia\\', \\'Estrela\\', \\'Quasar\\', \\'Upper Bound\\', \\'Galáxia\\', \\'Estrela\\', \\'Quasar\\' ] 24) # Transpõe o DataFrame para facilitar a visualização df_outlier = df_outlier.T # Exibe DataFrame de outliers df_outlier [31]: obj_ID alpha delta u g r i z run_ID rerun_ID \\\\ Outliers Total 0 0 0 55 98 132 198 319 0 0 Lower Bound 0 0 0 44 76 118 164 256 0 0 Galáxia 0 0 0 16 52 87 138 209 0 0 Estrela 0 0 0 27 22 29 25 45 0 0 Quasar 0 0 0 1 2 2 1 2 0 0 Upper Bound 0 0 0 11 22 14 34 63 0 0 Galáxia 0 0 0 4 15 5 13 38 0 0 Estrela 0 0 0 5 4 7 8 10 0 0 Quasar 0 0 0 2 3 2 13 15 0 0 cam_col field_ID spec_obj_ID redshift plate MJD fiber_ID Outliers Total 0 5390 0 8989 0 0 0 Lower Bound 0 0 0 0 0 0 0 Galáxia 0 0 0 0 0 0 0 Estrela 0 0 0 0 0 0 0 Quasar 0 0 0 0 0 0 0 Upper Bound 0 5390 0 8989 0 0 0 Galáxia 0 3548 0 34 0 0 0 Estrela 0 1102 0 0 0 0 0 Quasar 0 740 0 8955 0 0 0 Podemos notar que o DataSet possui outliers para os atributos de filtros fotométricos, para o redshift e para field_ID. Paraosfiltrosfotométricospodemosperceberumvalorprogressivononúmerodeoutliersdasbandas mais baixas (filtros mais baixos) até as mais altas, sendo cerca de 80% a 90% correspondentes a outliers abaixo do limite inferior. Esses valores não devem ser descartados; podem-se tratar de valores físicos reais (como estrelas próximas ou galáxias brilhantes, o que pode justificar a baixa concentraçãodessesoutliersclassificadoscomoquasares,jáquesetratamdeobjetosmaisdistantes). Para redshift nota-se um valor expressivo de outliers, cerca de 10% de toda a amostra, em que quaseatotalidade, comexceçãode34amostrasprovenientesdegaláxias, sãoquasareseestãoacima do limite superior. Trata-se de um fenômeno físico real, como havíamos previsto o grande número de quasares com altos redshifts, e, portanto, também serão importantes para o problema. Parafield_ID,comojáobservamospelarelaçãodeinformaçãomútua,esteatributopossuipouquís- sima relevância para prever a variável-alvo e, portanto, devemos descartá-la. 9 Pré-processamento dos Dados 9.1 Seleção de Atributos Queremos selecionar os principais atributos e descartar aqueles que são pouco relevantes para clas- sificar as classes. Como havíamos analisado, os atributos rerun_ID, cam_col e field_ID se mostraram pouco característicos nos gráficos de distribuição por classe, bem como na análise de informação mútua. 25[32]: # Descarta as colunas indesejadas df.drop(columns=[\\'rerun_ID\\', \\'cam_col\\', \\'field_ID\\'], inplace=True) 9.2 Transformação dos Dados Devido às diferentes propriedades e unidades físicas entre os atributos, iremos normalizar os dados para que todos estejam na mesma escala. Neste caso, o intervalo utilizado será [-1, 1], tal que o valor médio entre as amostras é nulo e o desvio padrão igual a 1. [33]: # Seleciona apenas colunas numéricas df_numeric = df.drop(columns=[\\'class\\']) # Normaliza os dados scaler = StandardScaler() df_normalized = scaler.fit_transform(df_numeric) # Converte de volta para DataFrame mantendo os nomes das colunas originais df_normalized = pd.DataFrame( df_normalized, columns=df_numeric.columns, index=df.index ) # Recupera a coluna da variável-alvo df_normalized[\\'class\\'] = df[\\'class\\'] 9.3 Redução de Dimensionalidade Para tratar das variáveis redundantes, vamos utilizar PCA (Análise de Componentes Princi- pais), que consiste em transformar variáveis correlacionadas em um conjunto menor de variáveis não correlacionadas, reduzindo a dimensionalidade sem perder muita informação. [34]: # Seleciona as variáveis redundantes variaveis_redundantes_1 = df_normalized[[\\'spec_obj_ID\\', \\'plate\\']] variaveis_redundantes_2 = df_normalized[[\\'obj_ID\\', \\'run_ID\\']] # Cria objeto pca para reduzir a uma única dimensão pca = PCA(n_components=1) # Aplica PCA nas variáveis redundantes pca_1_resultado = pca.fit_transform(variaveis_redundantes_1) pca_2_resultado = pca.fit_transform(variaveis_redundantes_2) # Cria DataFrames com os resultados do PCA, preservando os índices originais df_pca_1 = pd.DataFrame(pca_1_resultado, columns=[\\'PCA_1\\'], index=df.index) df_pca_2 = pd.DataFrame(pca_2_resultado, columns=[\\'PCA_2\\'], index=df.index) # Concatena os resultados ao DataFrame original e remove colunas redundantes df_reduced = pd.concat( [df_normalized, df_pca_1, df_pca_2], axis=1 ).drop( columns=[\\'spec_obj_ID\\', \\'plate\\', \\'obj_ID\\', \\'run_ID\\'] ) df_reduced.head() 26[34]: alpha delta u g r i z \\\\ 0 -0.434597 0.425517 0.798798 0.806782 0.403953 0.046001 0.013999 1 -0.339915 0.363391 1.198064 1.079967 1.584395 1.185087 1.611170 2 -0.367244 0.582702 1.413732 0.997513 0.519736 0.150012 0.101520 3 1.669522 -1.249122 0.024940 1.543642 1.059894 0.807601 0.272435 4 1.737308 -0.150255 -1.174337 -1.497665 -1.697426 -1.767888 -1.825836 redshift MJD fiber_ID class PCA_1 PCA_2 0 0.079549 0.423198 -1.021353 GALAXY 0.323309 0.630186 1 0.277088 1.420719 -0.081893 GALAXY 2.542624 -0.026409 2 0.092415 0.001850 -0.551623 GALAXY -0.268752 0.630186 3 0.486761 1.354918 1.195186 GALAXY 1.921834 0.208336 4 -0.630273 0.330855 1.441060 GALAXY 0.471365 -2.606091 9.4 Codificação de Variáveis Categóricas Precisamos realizar a divisão dos dados de treino e de teste. Antes, vamos codificar o atributo de saída para o tipo numérico. [35]: # Codifica a coluna \\'class\\' label_encoder = LabelEncoder() df_reduced[\\'class\\'] = label_encoder.fit_transform(df_reduced[\\'class\\']) # Verifica o mapeamento print(\"Mapeamento das classes:\") for i, class_name in enumerate(label_encoder.classes_): print(f\"{class_name} → {i}\") Mapeamento das classes: GALAXY → 0 QSO → 1 STAR → 2 9.5 Divisão dos Dados Realizando a separação entre dados de teste e de treino. [36]: # Separa as variáveis X da variável-alvo y X = df_reduced.drop(columns=[\\'class\\']) y = df_reduced[\\'class\\'] # Divide em treino e teste X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) 9.6 Balanceamento dos Dados 9.6.1 Oversampling Aplicando oversampling e undersampling para lidar com o desbalanceamento das classes. [37]: # Aplica oversampling nos dados de treino sm = SMOTE(random_state=42) X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train) # Conta o novo n° de amostras para cada classe y_train_oversampled.value_counts() 27[37]: 2 41629 0 41629 1 41629 Name: class, dtype: int64 9.6.2 Undersampling [38]: # Aplica undersampling nos dados de treino rus = RandomUnderSampler(random_state=42) X_train_undersampled, y_train_undersampled = rus.fit_resample(X_train, y_train) # Conta o novo n° de amostras para cada classe y_train_undersampled.value_counts() [38]: 0 13219 1 13219 2 13219 Name: class, dtype: int64 10 Aprendizado de máquina 10.1 Modelos de Aprendizado de Máquina Considerando a complexidade do problema, o tamanho do conjunto de dados e a grande presença de outliers em certos atributos, é necessária a escolha de modelos mais robustos a outliers, maior capacidade de generalização e melhor eficiência computacional. 10.1.1 Random Forest O Random Forest é um modelo baseado em múltiplas árvores de decisão que usa a técnica de Bagging(BootstrapAggregating). Eletreinaváriasárvoresdeformaindependenteemsubconjuntos aleatórios dos dados e combina suas previsões. Isso reduz o overfittinge melhora a estabilidade do modelo. 10.1.2 XGBoost O XGBoost (Extreme Gradient Boosting) é uma versão otimizada do Gradient Boosting, projetada para alta eficiência e desempenho. Ele constrói árvores sequencialmente, corrigindo os erros da anterior, e usa técnicas como regularização L /L e poda de árvores para evitar overfitting. 1 2 Seu paralelismo e otimizações tornam-no muito eficiente em grandes DataSets. 10.1.3 LightGBM O LightGBM (Light Gradient Boosting Machine) é uma alternativa ao XGBoost, mas com focoemmaiorvelocidadeeeficiência. EleutilizaatécnicaHistogram-basedLearning,ondeosdados são agrupados em bins antes do treinamento, acelerando a construção das árvores e reduzindo o uso de memória. É ideal para grandes volumes de dados. 10.1.4 CatBoost O CatBoost é uma versão do Gradient Boosting desenvolvida pela Yandex, com melhorias no tratamento de variáveis categóricas. Ele usa técnicas como Ordered Boosting, que evita overfitting 28aoordenarosdadosdeformaespecífica,epermitecapturarmelhorinterdependênciasentrevariáveis sem necessidade de pré-processamento complexo. 10.1.5 Justificativa da Escolha dos Modelos Estes modelos foram escolhidos devido às suas seguintes características: • Robustez a outliers e dados correlacionados: Esses modelos conseguem lidar bem com os valores extremos de redshift e minimizar o impacto de variáveis correlacionadas, reduzindo o risco de overfitting. • Eficiência computacional e escalabilidade: Também, possuem otimizações que aceleram o treinamento, tornando-os ideais para conjuntos de dados grandes, como o que temos. • Alta capacidade de generalização: Ao corrigirem iterativamente os erros das árvores anteriores e aplicarem regularizações, esses modelos encontram um equilíbrio entre viés e variância, melhorando a precisão da classificação. • Versatilidade e desempenho superior: O uso de diferentes técnicas de boosting garante alta acurácia, permitindo capturar padrões complexos e relações não lineares nos dados as- tronômicos. 10.2 Métricas de avaliação As principais métricas de avaliação de aprendizado de máquina para classificação de várias classes e as que iremos utilizar são as seguintes: 10.2.1 Matriz de Confusão A matriz de confusão é uma tabela que descreve o desempenho de um modelo de classificação, mostrando a distribuição das previsões feitas em comparação com as classes reais. Ela ajuda a entender quais erros o modelo está cometendo. Para um problema de classificação multiclasse, a matriz de confusão tem a seguinte estrutura: Predito: Classe 1 Predito: Classe 2 Predito: Classe 3 Real: Classe 1 C C C 11 12 13 Real: Classe 2 C C C 21 22 23 Real: Classe 3 C C C 31 32 33 Onde: - C representa o número de exemplos da classe real $ i $ que foram classificados como ij classe predita j. - Os valores da diagonal principal $C_{11}, C_{22}, C_{33} $ representam as classificações corretas (acertos). - Os valores fora da diagonal representam erros de classificação (falsas previsões). Essamatrizpermiteanalisarquaisclassesomodeloconfundemaisepodeserutilizadaparacalcular métricas como acurácia, precisão e revocação. 10.2.2 Accuracy (Acurácia) A acurácia é uma das métricas de avaliação mais simples e amplamente usadas para modelos de classificação. Ela mede a proporção de previsões corretas feitas pelo modelo em relação ao total de amostras. 29A acurácia é dada por: Número de previsões corretas Accuracy = (4) Número total de amostras A acurácia para o caso multiclasse pode ser expressa matematicamente como: (cid:80) C Accuracy = i ii (5) (cid:80) (cid:80) C i j ij onde C representa os valores da diagonal principal da matriz de confusão, ou seja, as previsões ii corretas para cada classe e (cid:80) (cid:80) C é o número total de amostras classificadas. i j ij 10.2.3 Precision (Precisão) A precisão é uma métrica que avalia a exatidão das previsões feitas pelo modelo. Ela calcula a proporção de exemplos classificados como pertencentes a uma determinada classe que realmente pertencem a essa classe. Para problemas de classificação multiclasse, a precisão para cada classe i é calculada como: C Precision = ii (6) i (cid:80) C j ji onde (cid:80) C é o total de previsões que o modelo fez para a classe i. j ji Para obter uma métrica geral (precisão macro, média das precisões por classe), utilizamos: 1 (cid:88) Macro Precision = Precision (7) i N i 10.2.4 Recall (Revocação) A revocação mede a capacidade do modelo de identificar corretamente todas as instâncias de uma determinada classe. Ela calcula a proporção de exemplos de uma classe que foram corretamente identificados. Para problemas de classificação multiclasse, a revocação para cada classe i é calculada como: C recall = ii (8) i (cid:80) C j ij onde (cid:80) C é o total de amostras que pertencem à classe i. j ij Para obter uma métrica geral (revocação macro), utilizamos: 1 (cid:88) Macro Recall = recall (9) i N i 3010.2.5 F1-Score O F1-score é uma métrica que combina precisão e recall em uma única medida, fornecendo um equilíbrio entre as duas. O F1-score é especialmente útil quando há uma distribuição desigual entre as classes, ou seja, em problemas com classes desbalanceadas. o F1-score é dado por Precision×recall F1-score = 2× Precision+recall Ou seja, é a média harmônica entre precisão e recall. Para obter uma métrica geral (F1-Score macro), utilizamos: 1 (cid:88) Macro F-1 Score = F-1 Score (10) i N i 10.2.6 AUC (Area Under the Curve) A AUC (Área Sob a Curva) é uma métrica que quantifica o desempenho de um modelo de classificação, especialmente em problemas binários. Ela mede a capacidade do modelo em distinguir entre as classes positiva e negativa em diferentes limiares de decisão. A AUC está relacionada à curva ROC (Receiver Operating Characteristic), que traça a taxa de verdadeiros positivos (TP) em relação à taxa de falsos positivos (FP). O valor da AUC varia de 0 a 1. Para obter uma métrica geral (AUC macro) utilizamos a técnica One-vc-rest, que consiste em calcularovalordeAUCparacadaclasse, considerando-acomopositivaemrelaçãoàsoutrasclasses, eemseguida,pode-secalcularumaAUCmédiaparaavaliarodesempenhogeralemtodasasclasses. 10.3 Treinamento dos Modelos 10.3.1 Configuração dos Modelos [39]: # Modelos escolhidos modelos = [ (\\'RandomForest\\', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)), (\\'XGBoost\\', XGBClassifier(n_estimators=100, random_state=42, n_jobs=-1)), (\\'LightGBM\\', LGBMClassifier(n_estimators=100, random_state=42, verbose=0, num_threads=-1)), (\\'CatBoost\\', CatBoostClassifier(n_estimators=100, random_state=42, verbose=0)) ] 10.3.2 Validação Cruzada com Oversampling Primeiro, treinando os modelos com as variáveis nas quais realizamos oversampling [40]: # Cria o objeto de validação cruzada cv = KFold(n_splits=5, shuffle=True, random_state=42) # Lista para armazenar resultados com oversampling e métricas a serem utilizadas resultados_oversampled = [] scoring = [\\'accuracy\\', \\'precision_macro\\', \\'recall_macro\\', \\'f1_macro\\'] 31# Aplica validação cruzada para cada modelo e coleta as métricas for nome, modelo in modelos: scores_oversampled = cross_validate( modelo, X_train_oversampled, y_train_oversampled, cv=cv, scoring=scoring, return_train_score=False ) resultados_oversampled.append(( nome, scores_oversampled[\\'test_accuracy\\'].mean(), scores_oversampled[\\'test_precision_macro\\'].mean(), scores_oversampled[\\'test_recall_macro\\'].mean(), scores_oversampled[\\'test_f1_macro\\'].mean(), scores_oversampled[\\'fit_time\\'].mean() )) [41]: # Cria um DataFrame para organizar os resultados df_resultados_oversampled = pd.DataFrame( resultados_oversampled, columns=[ \\'Modelo\\', \\'Acurácia Média\\', \\'Precisão Média\\', \\'Revocação Média\\', \\'F1 Média\\', \\'Tempo de Treinamento (s) / fold\\' ] ) # Exibe os resultados ordenados pela Acurácia Média df_resultados_oversampled.sort_values(by=\\'Acurácia Média\\', ascending=False) [41]: Modelo Acurácia Média Precisão Média Revocação Média F1 Média \\\\ 0 RandomForest 0.982192 0.982216 0.982191 0.982178 1 XGBoost 0.980590 0.980646 0.980587 0.980586 2 LightGBM 0.978461 0.978567 0.978457 0.978455 3 CatBoost 0.976939 0.977034 0.976932 0.976931 Tempo de Treinamento (s) / fold 0 23.341700 1 2.839957 2 1.668357 3 4.500238 Acima podemos ver a performance média de cada modelo treinado e seu correspondente tempo médio de treinamento para cada iteração da validação cruzada. Isso pode ser útil para comparar a eficiência dos modelos. Os modelos performaram bem para todas as métricas e realizaram em um tempo relativamente curto. 10.3.3 Validação Cruzada com Undersampling E agora treinando os modelos com os dados nos quais foram aplicados undersampling. 32[45]: # Lista para armazenar resultados com undersampling e métricas a serem utilizadas resultados_undersampled = [] # Aplica validação cruzada para cada modelo e coletando métricas for nome, modelo in modelos: scores_undersampled = cross_validate( modelo, X_train_undersampled, y_train_undersampled, cv=cv, scoring=scoring, return_train_score=False ) resultados_undersampled.append(( nome, scores_undersampled[\\'test_accuracy\\'].mean(), scores_undersampled[\\'test_precision_macro\\'].mean(), scores_undersampled[\\'test_recall_macro\\'].mean(), scores_undersampled[\\'test_f1_macro\\'].mean(), scores_undersampled[\\'fit_time\\'].mean() )) [46]: # Cria DataFrame para organizar os resultados df_resultados_undersampled = pd.DataFrame( resultados_undersampled, columns=[ \\'Modelo\\', \\'Acurácia Média\\', \\'Precisão Média\\', \\'Revocação Média\\', \\'F1 Média\\', \\'Tempo de Treinamento Médio (s) / Iteração\\' ] ) # Exibe os resultados df_resultados_undersampled.sort_values(by=\\'Acurácia Média\\', ascending=False) [46]: Modelo Acurácia Média Precisão Média Revocação Média F1 Média \\\\ 2 LightGBM 0.973750 0.973896 0.973754 0.973731 0 RandomForest 0.973195 0.973256 0.973214 0.973156 1 XGBoost 0.973119 0.973297 0.973131 0.973111 3 CatBoost 0.971733 0.971844 0.971741 0.971703 Tempo de Treinamento Médio (s) / Iteração 2 0.759198 0 5.593778 1 1.319102 3 1.921897 10.3.4 Comparação de Desempenho Entre as Técnicas Comopodemosver,obtivemosumaperformancemédiamelhoremtodososmodelosutilizandoatéc- nica de oversampling, com destaque para os modelos Random Forest e XGBoost que obtiveram 98,22% e 98,06% de acurácia média, 98,22 e 98,06, também, de F1-score médio, respectivamente, e com o tempo de execução de cada validação sendo 8 vezes menor para o XGBoost em relação 33ao Random Forest. Destaque também para os modelos LightGBM e CatBoost que obtiveram performances pouco abaixo, mas um tempo de treinamento muito bom, oferecendo um baixo custo computacional. 11 Otimização de Hiperparâmetros A otimização de hiperparâmetros é essencial para melhorar o desempenho do modelo e garantir que ele generalize bem para novos dados. Modelos de aprendizado de máquina possuem parâmetros internos ajustáveis que influenciam diretamente sua capacidade de aprender padrões e evitar prob- lemas como overfitting (excesso de ajuste) ou underfitting (subajuste), além de que modelos mais bem ajustados convergem mais rapidamente e podem consumir menos recursos computacionais. 11.1 Configuração dos Hiperparâmetros Iremos realizar esta etapa para os dois modelos com as melhores performances, o Random Forest e o XGBoost. Os dados de treino utilizados serão os quais implementamos a técnica oversampling, dada a sua performance superior em relação ao undersampling. [47]: # Hiperparâmetros para Random Forest rf_params = { \\'n_estimators\\': [100, 200], \\'max_depth\\': [10, 20, None], \\'min_samples_split\\': [2, 5, 10], \\'min_samples_leaf\\': [1, 2, 4], \\'bootstrap\\': [True, False], \\'max_features\\': [\\'sqrt\\', \\'log2\\'], \\'criterion\\': [\\'gini\\', \\'entropy\\'] } # Hiperparâmetros para XGBoost xgb_params = { \\'n_estimators\\': [100, 200, 500] , \\'learning_rate\\': [0.01, 0.03, 0.1, 0.2], \\'max_depth\\': [3, 6, 9], \\'subsample\\': [0.8, 0.9, 1.0], \\'colsample_bytree\\': [0.8, 0.9, 1.0], \\'gamma\\': [0, 0.1, 0.2, 0.5], \\'reg_lambda\\': [0, 1, 2, 5], \\'reg_alpha\\': [0, 0.1, 0.5, 1] } 11.2 Aplicação de Técnica para Otimização A técnica de otimização de hiperparâmetros que iremos utilizar é a RandomizedSearchCV, ou Busca Aleatória, que é uma otimização eficiente que amostra aleatoriamente um número fixo de combinações, diferentemente do GridSearchCV que testa todas as combinações possíveis dentro de um grid pré-definido. Ele é uma excelente alternativa para encontrar boas combinações, economizando tempo e recursos computacionais. 34[48]: # Dicionário de modelos e parâmetros modelos_otimizados = { \\'Random Forest\\': (RandomForestClassifier(random_state=42, n_jobs=-1), rf_params), \\'XGBoost\\': (XGBClassifier(random_state=42, n_jobs=-1), xgb_params) } # Dicionário para armazenar os melhores modelos melhores_modelos = {} # Loop para treinar e otimizar cada modelo for nome, (modelo, parametros) in modelos_otimizados.items(): start_time = time.time() # Inicia o cronômetro # Configuração do RandomizedSearchCV search = RandomizedSearchCV( modelo, parametros, cv=3, n_iter=20, scoring=\\'accuracy\\', n_jobs=-1, random_state=42 ) # Treinamento do modelo search.fit(X_train_oversampled, y_train_oversampled) # Armazenar o melhor modelo no dicionário melhores_modelos[nome] = search.best_estimator_ # Exibir resultados parciais print(f\"Modelo: {nome}\") print(f\"Melhores Parâmetros: {search.best_params_}\") print(f\"Tempo de Treinamento: {time.time() - start_time:.2f} segundos\") print(\"-\" * 127) Modelo: Random Forest Melhores Parâmetros: {\\'n_estimators\\': 100, \\'min_samples_split\\': 2, \\'min_samples_leaf\\': 1, \\'max_features\\': \\'log2\\', \\'max_depth\\': None, \\'criterion\\': \\'entropy\\', \\'bootstrap\\': False} Tempo de Treinamento: 1679.02 segundos ---------------------------------------------------------------------------------- Modelo: XGBoost Melhores Parâmetros: {\\'subsample\\': 1.0, \\'reg_lambda\\': 1, \\'reg_alpha\\': 0, \\'n_estimators\\': 500, \\'max_depth\\': 9, \\'learning_rate\\': 0.2, \\'gamma\\': 0, \\'colsample_bytree\\': 0.8} Tempo de Treinamento: 381.66 segundos ---------------------------------------------------------------------------------- É possível notar a diferença no tempo total de treinamento entre ambos os modelos, em que o treinamento do modelo Random Forest levou cerca de 4 vezes mais do que o modelo XGBoost, totalizando 1679,02s ou cerca de 28min no total. 3511.3 Avaliação dos Modelos XGBoost e Random Forest Otimizados VamostestarosmodelosotimizadosavaliandoprimeiramenteovalormédiodasmétricasAcurácia, Precisão, Revocação e F1-score. Posteriormente iremos avaliar os modelos com Matriz de Confusão e AUC. 11.4 Aplicação dos Modelos aos Dados de Teste [49]: # Lista para armazenar os resultados resultados_avaliacao = [] # Loop para avaliar cada modelo no conjunto de teste for nome, modelo in melhores_modelos.items(): # Fazer previsões no conjunto de teste y_pred = modelo.predict(X_test) # Calcular métricas de avaliação resultados_avaliacao.append({ \\'Modelo\\': nome, \\'Acurácia\\': accuracy_score(y_test, y_pred), \\'Precisão Média\\': precision_score(y_test, y_pred, average=\\'macro\\'), \\'Revocação Média\\': recall_score(y_test, y_pred, average=\\'macro\\'), \\'F1 Média\\': f1_score(y_test, y_pred, average=\\'macro\\') }) 11.5 Desempenho dos Modelos com Métricas Iniciais [50]: # Cria DataFrame para os resultados de avaliação df_avaliacao = pd.DataFrame(resultados_avaliacao) df_avaliacao = df_avaliacao.sort_values(by=\\'Acurácia\\', ascending=False) # Exibe o DataFrame com os resultados df_avaliacao [50]: Modelo Acurácia Precisão Média Revocação Média F1 Média 0 Random Forest 0.977567 0.974816 0.973743 0.974258 1 XGBoost 0.976600 0.974039 0.972363 0.973182 Para todas as métricas o modelo Random Forest teve uma performance média superior ao XG- Boost com uma diferença menor que 0,1%, levando um tempo tempo de treinamento total aproxi- madamente 4 vezes maior. 11.6 Importância dos Atributos Podemos visualizar a importância de cada atributo para a classsifcação dos objetos. [51]: # Importância dso atributos e matriz de confusão para cada modelo for nome, modelo in melhores_modelos.items(): y_pred = modelo.predict(X_test) print(f\"Modelo: {nome}\") # Importância das atributos feature_importance = modelo.feature_importances_ importancia_df = pd.DataFrame({ \\'Feature\\': X_train_oversampled.columns, \\'Importância\\': feature_importance 36}) importancia_df = importancia_df.sort_values(by=\\'Importância\\', ascending=False) plt.figure(figsize=(10, 5)) bars = plt.barh( importancia_df[\\'Feature\\'], importancia_df[\\'Importância\\'], color=\\'royalblue\\' ) plt.xlabel(\\'Importância\\') plt.ylabel(\\'Atributo\\') plt.title(f\\'Importância dos Atributos - {nome}\\') plt.gca().invert_yaxis() for bar in bars: plt.text( bar.get_width(), bar.get_y() + bar.get_height()/2, f\\'{bar.get_width():.2f}\\', va=\\'center\\') plt.show() Modelo: Random Forest Modelo: XGBoost 37TantoparaoXGBoostquantoparaoRandomForest,nota-seagrandesuperioridadeerelevância do atributo redshift para a classficação estelar, com uma pontação superior a 0,60. Por outro lado, todas os outros atrbutos possuem uma pontuação próxima a 0, ou seja, possuem baixa relevância para o problema. 11.7 Desempenho dos Modelos com Matriz de Confusão Podemos visualizar uma matriz de confusão e verificar o desempenho da previsão para cada classe. [52]: # Plot de matriz de confusão para cada modelo for nome, modelo in melhores_modelos.items(): y_pred = modelo.predict(X_test) cm = confusion_matrix(y_test, y_pred) plt.figure(figsize=(10, 5)) sns.heatmap( cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\\'Galáxia\\', \\'Quasar\\', \\'Estrela\\'], yticklabels=[\\'Galáxia\\', \\'Quasar\\', \\'Estrela\\'] ) plt.xlabel(\\'Predito\\') plt.ylabel(\\'Real\\') plt.title(f\\'Matriz de Confusão - {nome}\\') plt.show() 38OmodeloRandomForestotimizadoapresentaumbomdesempenhonaidentificaçãodeobjetos. Para as galáxias, foram registradas 17491 classificações corretas, com cerca de 1,5% das amostras classificadas como quasares e 0,33% classificadas como estrelas. No caso dos quasares, 5398 amostras foram corretamente classificadas, enquanto nãohouveclassificaçõesincorretas como classe 39estrelas. Entretanto, aproximadamente 5,99% das amostras foram incorretamente classificadas como galáxias. Quanto às estrelas, 6438 classificações foram corretas, com apenas 4 amostras sendo incorretamente classificadas como galáxias e nenhuma amostra classificada erroneamente como quasares. O modelo XGBoost otimizado apresenta um bom desempenho na identificação de objetos. Para as galáxias, foram registradas 17486 classificações corretas, com cerca de 1,5% das amostras classificadas como quasares e 0,34% classificadas como estrelas. No caso dos quasares, 5391 amostras foram corretamente classificadas, enquanto nãohouveclassificaçõesincorretas como classe estrelas. Entretanto, aproximadamente 6,11% das amostras de quasares foram incorretamente classificadas como galáxias. Quanto às estrelas, 6.421 classificações foram corretas, com 20 amostras sendo incorretamente classificadas como galáxias e 1 amostra classificada erroneamente como quasares. 11.8 Desempenho dos Modelos com ROC e AUC Por fim, utilizaremos a última métrica, a AUC com a técnica One-vs-Rest, possibilitando que AUC média seja calculada e avaliada em todas as classes. [53]: # Plot da curva ROC e AUC para cada modelo for nome, modelo in melhores_modelos.items(): y_pred = modelo.predict(X_test) y_proba = modelo.predict_proba(X_test) auc_mean = roc_auc_score(y_test, y_proba, multi_class=\\'ovr\\', average=\\'macro\\') print(f\"AUC média (One-vs-Rest) para {nome}: {auc_mean:.4f}\") for i in range(y_proba.shape[1]): fpr, tpr, _ = roc_curve(y_test == i, y_proba[:, i]) plt.plot(fpr, tpr, label=f\\'Classe {i}\\') plt.plot([0, 1], [0, 1], linestyle=\\'--\\', color=\\'gray\\', label=\\'Aleatório\\') plt.xlabel(\\'Taxa de Falsos Positivos (FPR)\\') plt.ylabel(\\'Taxa de Verdadeiros Positivos (TPR)\\') plt.title(f\\'Curva ROC Multiclasse - {nome}\\') plt.legend() plt.show() AUC média (One-vs-Rest) para Random Forest: 0.9953 40AUC média (One-vs-Rest) para XGBoost: 0.9956 41Como podemos ver, o valor médio obtido para cada modelo representa uma excelente capacidade de discriminação entre as classes, com ambos superando 0,99. Pelo gráfico, é evidenciado que ambos os modelos se saíram muito melhor do que uma classificação aleatória, não se configurando também como classificações por acaso. Os modelos aprenderam bem e se mostraram capazes de determinar as classes com alto desempenho. 12 Conclusão ORandom Forestotimizadoapresentouomelhordesempenhogeral, comumaacuráciade97,76% nos dados de teste, superando o XGBoost (97,66%). No entanto, o tempo de treinamento do Random Forest foi aproximadamente quatro vezes maior, o que pode ser um fator limitante em cenários onde a eficiência computacional é crítica. Também, o atributo redshift mostrou-se fundamental para a classificação, sendo o atributo mais relevante entre todos os atributos analisados. A avaliação robusta do modelo, utilizando múltiplas métricas como Acurácia, Precisão, Revo- cação, F1-score, Matriz de Confusão e AUC, garantiu que o modelo não sofreu overfitting e apresentou um desempenho consistente. No entanto, observamos que o modelo ainda pode ser aprimorado, especialmente na classificação de quasares, onde houve uma pequena taxa de erro. Para futuras melhorias, sugerimos a aplicação de técnicas como PCA nos filtros espectrométricos 42para reduzir a dimensionalidade e viabilizar otimizações mais avançadas, como GridSearch ou Optuna. Além disso, o deploy do modelo em um ambiente de produção permitiria sua utilização em tempo real, facilitando a classificação de objetos celestes em observações astronômicas. 43'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "minhas_stop_words = {'a', 'e', 'i', 'o', 'u'} # Juntando minhas stopwords com as já definidas no stopwords\n",
    "\n",
    "stop_words = api_stop_words | minhas_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "def tratamento_pln(text):\n",
    "\n",
    "    # 1. normalização: todos os caracteres de minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. remoção de números, pontuações e caracteres especiais\n",
    "    text = re.sub(r'[^a-zA-Záéíóú\\s]','', text) # na regex estão as exceções \n",
    "\n",
    "    # 3. tokenização com spacy \n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    # 4. remoção de stopwords, remoção de pontuação\n",
    "    # e Lematização (clean_tokens = tokens lematizados e sem stopwords)\n",
    "    clean_tokens = [token.lemma_ for token in doc if token.text not in stop_words and not token.is_punct]\n",
    "\n",
    "    clean_text = ' '.join(clean_tokens)\n",
    "\n",
    "    return clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do conjunto stop_words: 209 \n",
      "Stop_words ordenadas: \n",
      " ['a', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'até', 'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', 'do', 'dos', 'e', 'ela', 'elas', 'ele', 'eles', 'em', 'entre', 'era', 'eram', 'essa', 'essas', 'esse', 'esses', 'esta', 'estamos', 'estar', 'estas', 'estava', 'estavam', 'este', 'esteja', 'estejam', 'estejamos', 'estes', 'esteve', 'estive', 'estivemos', 'estiver', 'estivera', 'estiveram', 'estiverem', 'estivermos', 'estivesse', 'estivessem', 'estivéramos', 'estivéssemos', 'estou', 'está', 'estávamos', 'estão', 'eu', 'foi', 'fomos', 'for', 'fora', 'foram', 'forem', 'formos', 'fosse', 'fossem', 'fui', 'fôramos', 'fôssemos', 'haja', 'hajam', 'hajamos', 'havemos', 'haver', 'hei', 'houve', 'houvemos', 'houver', 'houvera', 'houveram', 'houverei', 'houverem', 'houveremos', 'houveria', 'houveriam', 'houvermos', 'houverá', 'houverão', 'houveríamos', 'houvesse', 'houvessem', 'houvéramos', 'houvéssemos', 'há', 'hão', 'i', 'isso', 'isto', 'já', 'lhe', 'lhes', 'mais', 'mas', 'me', 'mesmo', 'meu', 'meus', 'minha', 'minhas', 'muito', 'na', 'nas', 'nem', 'no', 'nos', 'nossa', 'nossas', 'nosso', 'nossos', 'num', 'numa', 'não', 'nós', 'o', 'os', 'ou', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'por', 'qual', 'quando', 'que', 'quem', 'se', 'seja', 'sejam', 'sejamos', 'sem', 'ser', 'serei', 'seremos', 'seria', 'seriam', 'será', 'serão', 'seríamos', 'seu', 'seus', 'somos', 'sou', 'sua', 'suas', 'são', 'só', 'também', 'te', 'tem', 'temos', 'tenha', 'tenham', 'tenhamos', 'tenho', 'terei', 'teremos', 'teria', 'teriam', 'terá', 'terão', 'teríamos', 'teu', 'teus', 'teve', 'tinha', 'tinham', 'tive', 'tivemos', 'tiver', 'tivera', 'tiveram', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivéramos', 'tivéssemos', 'tu', 'tua', 'tuas', 'tém', 'tínhamos', 'u', 'um', 'uma', 'você', 'vocês', 'vos', 'à', 'às', 'é', 'éramos']\n"
     ]
    }
   ],
   "source": [
    "# Visualizando stop words\n",
    "print(\"Tamanho do conjunto stop_words:\", len(stop_words), \"\\nStop_words ordenadas: \\n\", sorted(list(stop_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_pdf_text = tratamento_pln(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do texto em caracteres: 64064\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamanho do texto em caracteres:\",len(pdf_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'classificao objeto celeste february     introduo característica espectral so fundamental diferenciar distinto tipo objeto celeste estrela galáxia quasar estrela emit radiao eletromagnético maneira semelhante corpo negro exibir linha absoro específico caso galáxia espectro resultado soma espectro todo estrela demais material radiante compir quasar outro lado apresentar linha emisso marcante frequentemente acompanhar desvio expressivo vermelho redshift   motivao contexto dataset osloan digital Sky survey sdsséumprojetodepesquisaastronmicabaseadoemimagem conduzir telescópio grande angular   metro localizar observatório Apache point novo méxico Estados Unidos objetivo projeto mapear quarto céu visível obter observaes aproximadamente   milhe objeto espectro milho obje tos levantamento incluir informaes espectral fotométrica todo objeto astronmico detectar incluir estrela galáxia quasar dado so publicar periodicamente disponibilizar publicamente conjunto dado corresponder dado data release   dr     importncia estudo categorizar objeto astronmico base característica espectral fotométrica essencial entender evoluo estelar formao galáxia anális espectral revelar posio temperatura estrela enquanto característica fotométrica fornecer dado sobre luminosidade distncia juntas informaes ajudar mapear história universo processo moldar estrutura   dado   objid identificador objeto valor único identificar objeto catálogo ima gem utilizar arquivo dado sloan digital Sky survey sdss   alpha ngulo ascenso reta grau época j sistema coordenada amplamente utilizar astronomia descrever posio objeto celeste céu   delta ngulo declinao grau época j outro sistema coordenada comumente usar astronomia localizar objeto celeste   g r z filtro fotométrico usar sistema sdss medir quantidade luz emitir objeto diferentes faixa comprimento onda cada filtro corresponder cor específico luz ultravioleta g verde r vermelho infravermelho próximo z infravermelho   runid número execuo identificar varredura específico céu realizar sdss cada varredura cobrir regio determinar céu receber número único   rerunid número reprocessamento indicar imagem processar incluir verso software método calibrao utilizar criao imagem   camcol coluna cmera usar identificar linha varredura dentro execuo cada varredura divider múltiplo coluna cmera cobrir área maior céu   fieldid número campo utilizar identificar cada campo dentro varredura campo representar regio pequeno dentro coluna cmera   specobjid identificador único objeto espectroscópico óptico significar dois observaes diferente specobjid dever pertencer   mesmo classe objeto poder galáxia estrela quasar   class classe objeto classificao atribuir objeto base propriedade espectral poder galáxia estrela quasar   redshift valor desvio vermelho basear aumento comprimento onda luz emitir objeto devido movimento relaor observador valor medir expanso universo desde momento luz emitir objeto   plate identificador placa utilizar pesquisa espectroscópico sdss cada placo contém múltiplo fibra coletar luz diferentes objeto   mjd data juliano modificada utilizar indicar determinar conjunto dado sdss coletar verso modificado data juliano sistema padronizar representar data horário astronomia   fiberid identificador Fibra óptico direcionar luz plano Focal durante cada observao cada fibra coletar luz objeto diferente permitir sdss observer vários objeto simultaneamente informaes sobre atributo de este dataset poder encontrar aqui    atributo alvo classe   classe problema estrela galáxia quasar   estrela estrela so enorme esfera gás quente compor principalmente hidrognio hélio formar partir colapso nuvem molecular frio densa onde gravidade reunir matéria criar protoestrela eventualmente iniciar fuso nuclear duranteafaseprincipaldavidadeumaestrelaafusodohidrognioemhéliomantémseuequilíbrio contra gravidade estrela baixa massa brilhar trilhe ano enquanto massiva consumir combustível rapidamente viver apenas milhe ano final vida esgotamento hidrognio fazer estrela expandir estrela baixa masso tornar gigante vermelho terminar ans branca cercar nebuloso planetária estrela massivo sofrer fuso formar ferro colapsar exploder supernova deixar trás estrela nutrom buraco negro material expeler explose enriquecer futuro geraes estrela    galáxia galáxia so estrutura gigantesco compor estrela planeta vasto nuvem gás poeira todo manter junto gravidade variar tamanho desde pequeno algum milhar estrela gigante poder conter trilhe estrela medir milho anosluz maioria grande galáxia abriga buraco negro supermassivo centro elasapresentamdiferentesformassendoasmaiscomunsasespiraiseelípticasalémdasirregular possuir aparncia menos organizar maioria galáxia   bilhe   bilhe ano algum ser quase to antigo quanto próprio universo galáxia jovem conhecer formousar cerca   milhe ano galáxia poder organizar grupo   membro manter Unidos gravidade estrutura grande chamado aglomerar poder conter milhar galáxia vez poder formar superaglomerado so gravitacionalmente ligar conjunto superar glomerar vazio cósmico grande estrutura formar teio cósmico universo    quasar quasar so núcleo extremamente brilhante galáxia ativo alimentar buraco negro supermassivo consumir grande quantidade matéria matéria forma disco acreo redor buraco negro onde frico aquecer gás milhes grau emitir intenso radiao parte material ejetar jato colimar campo magnético universo primitivo fluxo gás cósmico alimentar buraco negro central tarde colise galáxia direcionar gás buraco negro ativar quasar quasar poder brilhar   vez via láctea apesar disso disco acreo apenas algum centena milhar unidade astronmico comparar   anosluz via lácteo    atributo físico   filtro fotométrico filtro fotométrico sloan digital sky survey sdss so elemento óptico selecionar faixa específico espectro eletromagnético permitir observao objeto celeste difer ente comprimento onda so fundamental caracterizar composio temperatura idade estrela galáxio outro corpo celeste so colocar frente detector ccd cmera sdss transmitir apenas luz faixa específico comprimento onda enquanto bloque outro frequncia eficincia transmisso depender material revestimento filtro ser calibrar garantir medie consistente comparável diferente observaesr   cada filtro captura característica físico específico objeto astronmico   filtro ultravioleta próximo   nm detectar estrela jovem quente pois emitar fortemente uv útil estudar formao estelar presena gás ionizar galáxia   filtro g verdeazular   em m sensível   emisso estrela intermediário contémr linha absoro cálcio ca ii hk importante classificar estrela segundo temperatura idade   filtro r vermelho   nm incluir linha emisso hidrognio ionizar h   em m traador formao estelar galáxia usar diferenciar populaes estelar jovem antigo   filtro infravermelho próximo   nm capta estrela frio regie rico poeira interestelar essencial estudar estrutura galáxia espiral Populaes estelar evoluír   filtro z infravermelho   nm permitir deteco objeto alto Redshifts pois luz visível galáxia distante deslocar paraoinfravermelho útilparaestudaraevoluocósmicadasgaláxia essesfiltrossoprojetar cobrir diferente parte espectro visível infravermelho próximo otimizar fotometria larga escala estudo astrofísico   redshift oredshiftéumparmetrofundamentalnaastronomiaecosmologiar quemedeodeslocamentodo linha espectral objeto comprimento onda grande devido   expanso universo definir    z   observar emitido    emitir onde   comprimento onda original luz   comprimento onda meder emitir observar espectro objeto redshift diretamente relacionar   distncia objeto devido   lei hubble estabelecer velocidade recesso galáxia proporcional   distncia v   h d    onde h constante hubble v velocidade recesso objeto d distncia v z   relacionar fórmula efeito doppler relativístico cid vc z    vc tal c velocidade luz vácuo portanto quanto grande redshift distante antigo luz observar objeto permitir estudar diferente época evoluo universo    coordenar alpha deltar representar coordenada equatorial objeto observar so usado indicar posio objeto céu assim coordenada latitude longitude mapa terrestre ascensso reta ra right ascension   coordenar equivalente   longitude mapa terrestre céu medir posio objeto longo linha equador celestir variar     adeclinao decar éacoordenadaequivalentelatitudenosistemadecoordenadasterrestre indicar posio objeto longo eixo perpendicular   linha equador celeste distncia objeto relao equador celestial variar       objetivo estudo objetivo de este projeto classificao objeto celeste estrela star galáxia galaxy quasar qso utilizar diferentes modelo classificao supervisionado métrico avaliao   importao biblioteca pacote    verso python utilizar python version python     importar dependncia importr panr pd importr numpy np importr sklearn importr time importr astropyunit importr Matplotlibpyplot plt pltrcparamsfontfamilyr   serif pltrcparamsaxeslabelsize    importr seaborn sns snssetstyledarkgrid snssetpaletteff ff ff importr Warnings Warningsfilterwarningsignore   importar classe fune específico from xgboost importr xgbclassifier from sklearnensemble importr   randomforestclassifier gradientboostingclassifier   from lightgbm importr lgbmclassifier from catboost importr catboostclassifier from sklearnpreprocessing importr   labelencoder standardscaler   from sklearnmodelselection importr   traintestsplit randomizedsearchcv Learningcurve Kfold crossvalidate   from imblearnoversampling importr smote from imblearnundersampling importr randomundersampler from Sklearnmetrics importr   accuracyscore confusionmatrix precisionscore recallscore fscorer roccurvir auc precisionrecallcurvar   from Sklearnpipeline importr pipelinir from astropycoordinate importr skycoord from Sklearndatasets importr makeclassification from Sklearnfeatureselection importr mutualinfoclassif from sklearndecomposition importr pca matplotlib inlinir   análise exploratório dado eda   carregamento dado    carregar datasetr df   pdreadcsvstarclassificationcsv    visualizar   primeiro linha dataframe dfhead   objid alpha delta g r                                      z runid rerunid camcol fieldid specobjid                                          Class redshift plate mjd fiberid   galaxy       galaxy       galaxy       galaxy       galaxy      primeiramente poder notar cerca   atributo dataset so apenas identificador único além data coleta atributo fotométrico valor redshift coordenado equa torial classe objeto questo classificar relevncia cada atributo dever estudar especial atributo indentificao    analisar dimense dataframe dfshape     dataset possuir   amostra distribuír   classe objeto astronmico cada objeto catálogo ser descrever   atributo featur além   coluna identificar respectivo categoria    destacar tipo cada atributo dfinfo class pandascoreframedataframe rangeindex   entrie   to   data columns total   columns   column nonnull count dtype       objid   nonnull float   alpha   nonnull float   delta   nonnull float     nonnull float   g   nonnull float   r   nonnull float     nonnull float   z   nonnull float   runid   nonnull int   rerunid   nonnull int   camcol   nonnull int   fieldid   nonnull int   specobjid   nonnull float   class   nonnull object   redshift   nonnull float   plate   nonnull int   mjd   nonnull int   fiberid   nonnull int dtypes float int object memory usage   mb dentrar   atributo apenas atributo class tipo object ser restante tipo int float codificao de esse atributo tratar posteriormente    determinar presena valor ausente dfisnullsum   objid   alpha   delta     g   r     z   runid   rerunid   camcol   fieldid   specobjid   class   redshift   plate   mjd   fiberid   dtype int valor ausente em este dataset portanto nenhum técnica preenchimento remoo dado necessário   análise descritivo    descrever estatística resumir dfdescribir   objid alpha delta   count     mean     std     min                    max     g r z   count      mean      std      min                     max      runid rerunid camcol fieldid specobjid   count      mean      std      min                        max      redshift plate mjd fiberid count      mean      std      min                     max      base estatística resumir poder tirar algum insights   ascenso reta alpha variar     fazer sentido pois cobrir todo faixa possível céu possuir média próximo   sugerir dado poder concentrado certo regie   adeclinaodeltavaideaentocobreumagrandepartedocéumasnocom pletamente média próximo   indicar objeto hemisfério norte celester   relao filtro g r z algum magnitude possuir valor anmalo   poder tratar placeholder dado ausente dever localizálo imediatamente descartálo afetem análises posterior filtro seguir valor médio próximo magnitude mínimo máxima mostrar objeto brilhante fraco cobrir grande faixa brilho   redshift possuir média próximo   indicar grande parte objeto er relativamente distante valor variar     pequeno negativo poder erro medio objeto movimento peculiar máximo   altíssimo sugerir presena quasar extremamente distante   mjd indicar observaes variar dentro grande faixa data   rerunid possuir desvio padro nulo todo outro estátistica igual   evidenciar todo   amostra atributo possuir valor portanto poder conter informao relevante estudo vamosverificaraconsistnciafísicadosdadoseindentificarvaloresquenosofisicamentepossível em este dataset    criar dicionário condie físico admitir Condicoes     alpha dfalpha     dfalpha     delta dfdelto     dfdelta     dfu     g dfg     r dfr     dfi     z dfz      validar presena condie retorner índice correspondente colunar condicao in condicoesitem indicesinvalir   dfindexcondicaotolist if indicesinvalir print fcoluna possuir lenindicesinvalir valor inválir índice   f joinmapstr indicesinvalir   else printfcoluna dentro limite físico   alpha dentro limite físico   delta dentro limite físico   possuir   valor inválir índice    g possuir   valor inválir índice    r dentro limite físico   dentro limite físico   z possuir   valor inválir índice   havíamos prever tratase placeholder erro medio atributo g z linha   dever descartar linha imediatamente    decarta valor índice   reindexa dado df   dfdropindex df   dfresetindexdroptrue   visualizao dado   distribuio classe vejamos distribuio contagem amostra cada classe    plot distribuio contagem amostra classe pltfigurefigsize snscountplotxdfclass plttitledistribuio classe   text   distribuio classe    contagem amostra classe dfclassvaluecounts   galaxy   star   qso   name class dtype int poder ver datasetr desbalanceado cerca trs vez galáxia estrela quasar lidar ir utilizar técnica oversampling undersar pling técnica oversampling envolver aleatoriamente selecionar exemplo classe minoritár substituir adicionar exemplo dataset treino outro lado undersampling con sistar remover aleatoriamente exemplo classe majoritário equilibrar distribuio classe abordagem ajudar melhorar desempenho modelo lidar dataset desbalancear vejamosasdistribuiesdosatributosemumgráficodedensidadeparacadaclassedavariávelalvo    criar objeto receber valor amostra pequeno classe possuir minamostra   de fclassvaluecountsmin   criar dataframe balancear agrupar class dfbalancear   dfgroupbyclasssamplenminamostro randomstate   receber lista atributo número linha coluna subplots   retornar subplots estimar funo densidade probabilidade def kdeplotbalanceadoatributo lir col fig axes   pltsubplotslinha col figsize   axe   axesflatten atributo in enumerateatributo snskdeplot datadfbalancear xatributo hueclass fillfalse axaxesi   axesisettitleatributo plttightlayout    criar listo atributo Identificao atributo   mjd fiberid camcolr fieldid   chamar funo kdeplotbalancear kdeplotbalanceadoatributo    poder ver distribuio atributo camcol fieldid possuir comportamento suficientemente distinguível classe primeiro ter distribuio uniforme periódico segundo assimétrica ter pico comum trs classe    criar outro lista atributo identificao restante atributo   plate objid runid specobjid   chamar funo kdeplotbalancear kdeplotbalanceadoatributo    todo atributo parecer possuir relao classificao classe cada distribuio comportar maneira característico notase semelhana dis tribuie atributo objid runid bem specobjid plate compor tamento mostrar acima parecer conter mesmo informaes outro bom evidenciar estudar correlao    criar listo atributo natureza físico atributo   redshift g r z   chamar funo kdeplotbalancear kdeplotbalanceadoatributo    devido valor alto redshift baixo g z ire transformálo escala logarítmico bom visualizao distribuie    transformar amostra atributo redshift escala logarítimico dfbalanceadoredshift   nplogdfredshift    chamar funo kdeplotbalancear kdeplotbalanceadoatributo    podemosnotarquetodososfiltrosfotométricospossuemdistribuiescaracterísticasedistinguível classe destaque resdhift possuir pico densidade regie diferente cada classe mostrar tratar atributo grande potencial classificao objeto estelar    criar listo coordenada equatorial atributo   alpha deltar   chamar funo kdeplotbalancear kdeplotbalanceadoatributo    coordenada equatorial parecer ter comportamento distinguível classe regie grande pequeno densidade semelhante si   análise correlao   correlao pearson fim determinar necessidade reduzir dimensionalidade conjunto atributo ire  reproduzir mapa calor correlao pearson entender atributo correlacionar linearmente si amatrizéinterpretadadaseguinteformar indicaumamáximo dependncia linear positivo significar   medida variável aumentar outro aumentar forma linear    indicar grande dependncia linear negativo significar   medida variável aumentar outro diminui forma linear    indicar dependncia linear nular significar relao linear dois variávei entanto importante destacar significar exista qualquer tipo relao variávei poder ter correlao linear    matriz correlao descartar variávelalvo   identificador pouco relevante pltfigurefigsize corr   dfdropcolumnsclassrerunid camcol fieldidcorr Snsheatmapcorr Annottrue cmapblue fmtf linewidths plttitlematriz correlao pltshow mencionar anteriormente curva distribuio densidade correlao linear positivo igual   plate specobjid bem objid runid refora redundncia total informao possuir si poder notar alta correlao linear positivo filtro fotométrico destaque paraasbandaszeizeriereregassimcomoentreplateemjdosatributosdecoordenado equatorial possuir correlao baixa próximo   todo outro atributo assim redshift possuir correlao positivo torno     filtro fotométrico plate mjd   Verificao atributo idntico vejamos plate specobjid bem objid runid possuer fato valor idn tico    verificar amostra so identificamente igual   retorno true verdadeiro false falso dfplateequalsdfspecobjid   false    verificar amostra so identificamente igual   retorno true verdadeiro false falso dfobjidequalsdfrunid   false comopodemosver elesnopossuemvaloresidntico épossívelqueumavariávelsejaumaverso transformar outro exemplo pode logaritmo normalizar padronizar em este caso ir realizar gráfico disperso regresso linear comparar relao ajustir linear coincidir disperso dado esto apenas escalar portanto possuem informao idntico    ajustir linear disperso plate specobjid pltfigurefigsize Snsregplot datadf xplate yspecobjid scatterkwsalpha   Color black Linekwscolor gray   pltxlabelplate pltylabelspecobjid plttitledisperso regresso linear pltshow atributo plate specobjid possuem informao idntico    ajustir linear disperso runid objid pltfigurefigsize snsregplot datadf xrunid yobjid scatterkwsalpha   Color black Linekwscolor gray   pltxlabelrunid pltylabelobjid plttitledisperso regresso linear pltshow atributo objid runid possuir informao idntico ir estudar qual bom decise poder tomar quanto redundncia manter ambos variávei adiciona informao útil modelo apenas aumentar dimension alidade benefício   informao mútuar ir além capturar tanto relaes linear quanto linear ir utilizar método informao mútuo mutual information   mi consistir quantificar dependncia dois variávei saber quanto valor variável reduzir incerteza sobre outro quantificar quantidade informao variável contém sobre outro medida poder fazer entender relao atributo variávelalvo útil estudo    calcular informao mútuo cada atributo variávelalvo mi   mutualinfoclassif dfdropcolumnsclassr dfclassr discretefeaturesfalse    crian dataframe ordenar resultado midf   pddataframefeaturar dfdropcolumnsclasscolumns mi mi midf   midfsortvaluesbymi ascendingfalse midf   featurar mi   redshift    specobjid    objid    plate    mjd    runid    z    g          r    fiberid    deltar    alpha    rerunid    fieldid    camcol   poder ver notável alta relaor atributo redshift variávelalvo destaque atributo specobjid objid plate mjd enquanto atributo restante notase baixo relao relevncia determinao variávelalvo destaque filtro fotométrico coordenada equatorial contudo ir mantlo análises posteri or optarer descartar atributo rerunid fieldid camcolr dever   baixo relevncia demonstrar ter estudo   correlao coordenada equatorial variável alvo obter valor baixo correlao coordenada equatorial variávelalvo vamosrealizarumgráficodedispersodosobjetosporclasseemfunodeascensoretaedeclinao eobservarasregiesdocéuemqueasestrela quasaresegaláxiasforamobservadaspelotelescópio   cor   ff ff ff classes   dfclassunique   configurar estilo seaborn snssetstyledarkgrid   criar figura eixo pltfigurefigsize    plot disperso objeto classe in enumerateclasse subsetr   dfdfclass   classe pltscatter Subsetalpha subsetdeltar colorcoresi labelclar alpha   transparncia bom visualizao    adicionar rótulo título pltxlabelascenso reta   pltylabeldeclinao   plttitleascenso reta vs declinao pltlegendtitleclar pltgridtrue   mostrar gráfico pltshow pensandonaascensoretacomoonguloquetraaoequadorterrestre podesenotarapresenader duasregiescomaltaconcentraodeobjeto éevidenciadoqueacoordenadageográfico onde determinar objeto amostra catalogar pouco relevncia classe demonstrar regio central distribuio homognea galáxia quasar presena grande aglomerado estrela certo regie abaixo representao distribuio objeto utilizar astropy bom visualizao    criar figura subplot mollweidir classeunico   dfclassunique fig   pltfigurefigsize   ax   figaddsubplot projectionmollweidir classelabel in classeunico subset   dfdfclass   classelabel Coords   Skycoord rasubsetalpha   udegree decsubsetdelte   udegree frameicrs   axscatter coordsrawrapat   udegreeradian coordsdecradian s labelclasselabel alpha   axgridtrue axsettitle mapa celeste projeo mollweide fontsize   axlegend locupper Left fontsize bboxtoanchor   borderaxespad titleclar titlefontsizar markerscale    matplotliblegendlegend at xffb observar distribuio homogneo dois regie principal certos aglom erar estrela espaar dois regie ainda pouco evidente relao coordenada equatorial classificao objeto   correlao redshift variável alvo poder visualizar relao redshift atributo ir dar profundidade objeto céu   pltfigurefigsize    plot disperso objeto classe in enumerateclasse subsetr   dfdfclass   classe pltscatter subsetalpha Subsetredshift colorcoresi labelclar alpha   pltxlabelascenso reta   pltylabelredshift plttitleascenso reta vs redshift pltlegendtitleclar pltgridtrue pltshow   pltfigurefigsize    plot disperso objeto classe in enumerateclasse subsetr   dfdfclass   classe pltscatter subsetdelta subsetredshift colorcoresi labelclar alpha   pltxlabeldeclinao   pltylabelredshift plttitledeclinao vs redshift pltlegendtitleclar pltgridtrue pltshow notável atributo redshift possuir forte relevncia classificao classe nota distribuio bem determinar cada classe funo distncia telescópio grande concentrao quasar redshifts alto intermediário galáxia redshifts intermediário estrela redshifts próximo   possível notar em este dataset existir grande quantidade estrela próximo so jovem brilhante bem quasar distante so antigo ofuscado   identificao outliers vejar outliers em este amostra escala valor atributo diferente si realizar boxplot poder interessante identificar presena outlier portanto ir realizar seguinte abordagem    receber coluna atributo retorner valor abaixo   limite inferior acima limite superior   utilizar     quartil q q intervalo interquartil iqr def detectaoutlierscoluna q   Colunaquantile q   Colunaquantile iqr   q   q liminferior   q     iqr limsuperior   q     iqr outliersinferior   colunacoluna   liminferior outlierssuperior   colunacoluna   limsuperior return outliersinferior outlierssuperiorr    dicionário armazenar resultado outliers dadosoutlier     itera coluna dataframe exceto class col in dfdropcolumnsclassr inferior superior   detectaoutliersdfcolr   contag total outliers Totaloutliers   leninferior   lensuperior   contar outlier classe ambos limite classeinferiorcount   dflocinferiorindex classvaluecounts classesuperiorcount   dflocsuperiorindex classvaluecounts   armazena dado dicionário dadosoutliercolr    totaloutlier leninferior classeinferiorcountgetgalaxy   classeinferiorcountgetstar   classeinferiorcountgetqsor   lensuperior classesuperiorcountgetgalaxy   classesuperiorcountgetstar   classesuperiorcountgetqso     criar dataframe partir dicionário dfoutlier   pddataframefromdict dadosoutlier orientindex columns outliers total lower bound galáxiar estrela quasar upper bound galáxia estrela quasar     transpe dataframe facilitar visualizao dfoutlier   dfoutliert   exibir dataframe outlier dfoutlier   objid alpha delta g r z runid rerunid   outliers total            lower bound            galáxia            estrela            quasar            upper bound            galáxia            estrela            quasar            camcol fieldid specobjid redshift plate mjd fiberid outliers total         lower bound         galáxia         estrela         quasar         upper bound         galáxia         estrela         quasar         poder notar Dataset possuir outlier atributo filtro fotométrico redshift fieldid paraosfiltrosfotométricospodemosperceberumvalorprogressivononúmerodeoutliersdasbanr baixo filtro baixo alta ser cerca     correspondente outlier abaixo limite inferior valor dever descartar podemse tratar valor físico real estrela próximo galáxia brilhante poder justificar baixo concentraodessesoutliersclassificadoscomoquasaresjáquesetratamdeobjetosmaisdistante redshift notase valor expressivo outlier cerca   todo amostra quaseatotalidade comexceodeamostrasprovenientesdegaláxia soquasareseestoacima limite alto tratase fenmeno físico real havíamos prever grande número quasar alto Redshifts portanto sero importante problema parafieldidcomojáobservamospelarelaodeinformaomútuaesteatributopossuipouquís sim relevncia prever variávelalvo portanto dever descartála   préprocessamento dado   Seleo atributo querer selecionar principal atributo descartar so pouco relevante cla sificar classe havíamos analisar atributo rerunid camcol fieldid mostrar pouco característico gráfico distribuio classe bem análise informao mútuo    descartar coluna indesejar dfdropcolumnsrerunid camcol fieldid inplacetrue   transformao dado devido s diferentes propriedade unidade físico atributo ir normalizar dado todo mesmo escala em este caso intervalo utilizar    tal valor médio amostra nulo desvio padro igual     seleciona apenas coluna numérico dfnumericr   dfdropcolumnsclass   normalizo dado scaler   standardscaler dfnormalized   scalerfittransformdfnumeric   converte volta dataframe manter nome coluna original dfnormalized   pddataframe dfnormalized columnsdfnumericcolumns indexdfindex    recuperar coluna variávelalvo dfnormalizedclass   dfclassr   reduo dimensionalidade tratar variávei redundantes ir utilizar pca análise componente princi pai consistir transformar variável correlacionar conjunto pequeno variávei correlacionada reduzir dimensionalidade perder muito informao    selecionar variávei redundante variaveisredundante   dfnormalizedspecobjid plate variaveisredundante   dfnormalizedobjid runid   criar objeto pca reduzir único dimenso pca   pcancomponents   aplicar pca variávei redundante pcaresultar   pcafittransformvariaveisredundante pcaresultar   pcafittransformvariaveisredundante   criar dataframe resultado pca preservar índice original dfpca   pddataframepcaresultar columnspca indexdfindex dfpca   pddataframepcaresultar columnspca indexdfindex   concatenar resultado dataframe original removar coluna redundante dfreduced   pdconcat dfnormalized dfpca Dfpca Axis drop columnsspecobjid plate objid runid   dfreducedheadr   alpha delta g r z                                           redshift mjd fiberid class pca pca      galaxy        galaxy        galaxy        galaxy        galaxy     codificao variável categórico precisar realizar diviso dado treino teste antes ir codificar atributo saída tipo numérico    codificar coluna class labelencoder   labelencoder dfreducedclass   labelencoderfittransformdfreducedclass   verificar mapeamento printmapeamento classe classname in enumeratelabelencoderclasse printfclassname   mapeamento classe galaxy    qso    star     diviso dado realizar separao dado teste treino    separar variávei x variávelalvo y x   dfreduceddropcolumnsclass y   dfreducedclass   dividir treino teste xtrain xtest ytrain ytest   traintestsplitx y testsize randomstate   balanceamento dado   oversampling aplicar oversampling undersampling lidar desbalanceamento classe    aplico oversampling dado treino sm   smoterandomstate xtrainoversampled ytrainoversampled   Smfitresamplextrain Ytrain   contar novo n amostra cada classe ytrainoversampledvaluecounts         name class dtype int   undersampling    aplico undersampling dado treino rus   randomundersamplerrandomstate xtrainundersampled ytrainundersampled   rusfitresamplextrain Ytrain   contar novo n amostra cada classe ytrainundersampledvaluecounts         name class dtype int   aprendizar máquina   modelo aprendizado máquina considerar complexidade problema tamanho conjunto dado grande presena outliers certos atributo necessário escolha modelo robusto outlier grande capacidade generalizao bom eficincia computacional   random forest random forest modelo basear múltiplo árvore deciso usar técnica baggingbootstrapaggregating eletreinaváriasárvoresdeformaindependenteemsubconjunto aleatório dado combinar previse reduzir overfittinge melhorar estabilidade modelo   xgboost xgboost extremer Gradient boosting verso otimizar Gradient boosting projetar alto eficincia desempenho constrói árvore sequencialmente corrigir erro anterior usar técnica regularizao l l poda árvore evitar overfitting    paralelismo otimizaes tornamno eficiente grande datasets   lightgbm lightgbm light gradient boosting Machine alternativa xgboost focoemmaiorvelocidadeeeficincia eleutilizaatécnicahistogrambasedlearningondeosdar so agrupar bim antes treinamento acelerar construo árvore reduzir uso memória ideal grande volume dado   catboost catboost verso Gradient boosting desenvolver yandex melhoria tratamento variável categórico usar técnica ordered boosting evitar overfitting aoordenarosdadosdeformaespecíficaepermitecapturarmelhorinterdependnciasentrevariável necessidade préprocessamento complexo   justificativa escolha modelo modelo escolher devido s seguinte característica   robustez outlier dado correlacionar modelo conseguir lidar bem valor extremo redshift minimizar impacto variável correlacionar reduzir risco overfitting   eficincia computacional escalabilidade possuir otimizaes aceler treinamento tornandoos ideal conjunto dado grande   alto capacidade generalizao corrigir iterativamente erro árvore anterior aplicar regularizaes modelo encontrar equilíbrio viés varinciar melhorar preciso classificao   versatilidade desempenho superior uso diferentes técnica boosting garantir alto acurácia permitir capturar padre complexo relaes linear dado tronmico   métrico avaliao principal métrica avaliao aprendizado máquina classificao várias classe ir utilizar so seguinte   matriz confuso matriz confuso tabela descrever desempenho modelo classificao mostrar distribuio previse fazer comparao classe real ajudar entender qual erro modelo cometer problema classificao multiclar matriz confuso seguinte estrutura predito classe   predito classe   predito classe   real classe   c c c     real classe   c c c     real classe   c c c     onde   c representar número exemplo classe real     classificar ij classe predito j   valor diagonal principal c c c   representar classificaes correta acerto   valor diagonal representar erro classificao falso previses essamatrizpermiteanalisarquaisclassesomodeloconfundemaisepodeserutilizadaparacalcular métrico acurácia preciso revocao   accuracy acurácia acurácia métricas avaliao simples amplamente usar modelo classificao medir proporo previse correto fazer modelo relao total amostra acurácia dar número previse correto accuracy    número total amostra acurácia caso multiclar poder expressa matematicamente cid c accuracy   ii   cid cid c j ij onde c representar valor diagonal principal matriz confuso previse ii correta cada classe cid cid c número total amostra classificar j ij   precision preciso preciso métrica avaliar exatido previse fazer modelo calcular proporo exemplo classificar pertencente determinado classe realmente pertencer classe problema classificao multiclar preciso cada classe calcular c precision   ii   cid c j ji onde cid c total previse modelo fazer classe j ji obter métrico geral preciso macro médio precise classe utilizamos   cid macro precision   precision   n   recall revocao revocao medir capacidade modelo identificar corretamente todo instncia determinado classe calcular proporo exemplo classe corretamente identificar problema classificao multiclar revocao cada classe calcular c recall   ii   cid c j ij onde cid c total amostra pertencer   classe j ij obter métrico geral revocao macro utilizamos   cid macro recall   recall   n   fscorer fscore métrico combinar preciso recall único medida fornecer equilíbrio dois fscore especialmente útil distribuio desigual classe problema classe desbalancear fscore dar precisionrecall fscorer    precisionrecall média harmnico preciso recall obter métrico geral fscorer macro utilizamos   cid macro f score   f score   n   auc areo under the curver auc área sob curva métrica quantificar desempenho modelo classificao especialmente problema binário medir capacidade modelo distinguir classe positivo negativo diferentes limiar deciso auc relacionado   curvar roc receiver operating characteristic traa taxa verdadeiro positivo tp relao   taxa falso positivo fp valor auc variar     obter métrico geral auc macro utilizamos técnica onevcrest consistir calcularovalordeaucparacadaclar considerandoacomopositivaemrelaosoutrasclasse eemseguidapodesecalcularumaaucmédiaparaavaliarodesempenhogeralemtodasasclasse   treinamento modelo   configurao modelo    modelo escolher modelo    randomforest randomforestclassifiernestimators randomstate njobs xgboost xgbclassifiernestimators randomstate njobs Lightgbm Lgbmclassifiernestimators randomstate verbose numthreads catboost catboostclassifiernestimators randomstate verbose    validao cruzar oversampling primeiro treinar modelo variávei qual realizar oversampling    criar objeto validao cruzar cv   kfoldnsplits shuffletrue randomstate   listar armazenar resultado oversampling métrico ser utilizar resultadosoversampled    scoring   accuracy precisionmacro recallmacro fmacro   aplicar validao cruzar cada modelo coletar métrico nome modelo in modelo scoresoversampled   crossvalidate modelo xtrainoversampled ytrainoversampled cvcv Scoringscoring returntrainscorefalse   resultadosoversampledappend nome Scoresoversampledtestaccuracymean scoresoversampledtestprecisionmacromean Scoresoversampledtestrecallmacromean scoresoversampledtestfmacromean scoresoversampledfittimemean     criar dataframe organizar resultado dfresultadosoversampled   pddataframe resultadosoversampled columns modelo acuráciar médio preciso médio revocao médio f médio tempo treinamento s   fold     exibir resultado ordenar acurácia médio dfresultadosoversampledsortvaluesbyacurácia médio ascendingfalse   modelo acuráciar médio preciso médio revocao médio f média    randomforest       xgboost       lightgbm       catboost      tempo treinamento s   fold          acima poder ver performance médio cada modelo treinar correspondente tempo médio treinamento cada iterao validao cruzar poder útil comparar eficincia modelo modelo performar bem todo métrico realizar tempo relativamente curto   validao cruzar undersampling agora treinar modelo dado qual aplicar undersampling    listar armazenar resultado undersampling métrico ser utilizar resultadosundersampled     aplicar validao cruzar cada modelo coletar métrico nome modelo in modelo scoresundersampled   crossvalidate modelo xtrainundersampled Ytrainundersampled cvcv Scoringscoring returntrainscorefalse   resultadosundersampledappend nome Scoresundersampledtestaccuracymean scoresundersampledtestprecisionmacromean Scoresundersampledtestrecallmacromean scoresundersampledtestfmacromean scoresundersampledfittimemean     criar dataframe organizar resultado dfresultadosundersampled   pddataframe resultadosundersampled columns modelo acuráciar médio preciso médio revocao médio f médio tempo treinamento médio s   iterao     exibir resultado dfresultadosundersampledsortvaluesbyacurácia médio ascendingfalse   modelo acuráciar médio preciso médio revocao médio f média    lightgbm       randomforest       xgboost       catboost      tempo treinamento médio s   iterao           comparao desempenho técnica comopodemosverobtivemosumaperformancemédiamelhoremtodososmodelosutilizandoatéc nico oversampling destaque modelo random forest xgboost obtiver     acurácia médio     fscore médio respectivamente tempo execuo cada validao ser   vez pequeno xgboost relaor random forest destaque modelo lightgbm catboost obtiver performance pouco abaixo tempo treinamento bom oferecer baixo custo computacional   otimizao hiperparmetro otimizao hiperparmetro essencial melhorar desempenho modelo garantir generalizar bem novo dado modelo aprendizado máquina possuir parmetro interno ajustável influencir diretamente capacidade aprender padre evitar prob lema overfitting excesso ajustir underfitting subajustar além modelo bem ajustado convergem rapidamente poder consumir pouco recurso computacional   configurao hiperparmetro ir realizar etapa dois modelo bom performance random forest xgboost dado treino utilizar sero qual implementamos técnica oversampling dar performance alto relao undersampling    hiperparmetro random forest rfparams    nestimators    maxdepth    none minsamplessplitr     minsamplesleaf     bootstrap true false maxfeatur Sqrt log criterionr gini entropy    hiperparmetro xgboost xgbparams    nestimators      learningrate      maxdepth     subsample     colsamplebytree     gammar      reglambda      regalpha        aplicao técnica otimizao técnica otimizao hiperparmetro ir utilizar randomizedsearchcv busca aleatório otimizao eficiente amostra aleatoriamente número fixo combinaes diferentemente gridsearchcv testar todo combinaes possível dentro grid prédefiner excelente alternativa encontrar boas combinaes economizar tempo recurso computacional    dicionário modelo parmetro modelosotimizado    random forest randomforestclassifierrandomstater njobs rfparams xgboost xgbclassifierrandomstate njobs xgbparams    dicionário armazenar bom modelo melhoresmodelo     loop treinar otimizar cada modelo nome modelo parametro in modelosotimizadositem Starttime   timetime   iniciar cronmetro   configurao randomizedsearchcv search   randomizedsearchcv modelo parametro cv niter scoringaccuracy njobs randomstate    treinamento modelo searchfitxtrainoversampled ytrainoversampled   armazenar bom modelo dicionário melhoresmodelosnome   searchbestestimator   exibir resultado parcial printfmodelo nome printfmelhor parmetro searchbestparams printftempo treinamento timetime   starttimef segundo print    modelo random forest bom parmetro nestimators   minsamplessplitr   minsamplesleaf   maxfeatur log Maxdepth none Criterion entropy bootstrap false tempo treinamento   segundo   modelo xgboost bom parmetro subsample   reglambda   regalpha   nestimators   maxdepth   learningrate   gammar   colsamplebytree   tempo treinamento   segundo   possível notar diferena tempo total treinamento ambos modelo treinamento modelo random forest levar cerca   vez modelo xgboost totalizar s cerca min total   avaliao modelo xgboost random forest otimizar vamostestarosmodelosotimizadosavaliandoprimeiramenteovalormédiodasmétricasacurácia preciso revocao fscorer posteriormente ir avaliar modelo matriz confuso auc   aplicao modelo dado teste    listar armazenar resultado resultadosavaliacao     loop avaliar cada modelo conjunto teste nome modelo in melhoresmodelositem   fazer previse conjunto teste ypred   modelopredictxtest   calcular métricas avaliao resultadosavaliacaoappend modelo nome acuráciar accuracyscoreytest ypred preciso médio precisionscoreytest ypred averagemacro revocao médio recallscoreytest ypred averagemacro f média fscoreytest ypred averagemacro    desempenho modelo métrica inicial    criar dataframe resultado avaliao dfavaliacao   pddataframeresultadosavaliacao dfavaliacao   dfavaliacaosortvaluesbyacurácia ascendingfalse   exibir dataframe resultado dfavaliacao   modelo acurácia preciso médio revocao médio f médio   random forest       xgboost      todo métrico modelo random forest performance médio alto xg boost diferena pequeno   levar tempo tempo treinamento total aproxi madamente   vez grande   importncia atributo poder visualizar importncia cada atributo classsifcao objeto    importncia dso atributo matriz confuso cada modelo nome modelo in melhoresmodelositem ypred   modelopredictxtest printfmodelo nome   importncia atributo featureimportance   modelofeatureimportance importanciadf   pddataframe featurar xtrainoversampledcolumns importncia featureimportancer   importanciadf   importanciadfsortvaluesbyimportncia ascendingfalse pltfigurefigsize   bar   pltbarh importanciadffeature importanciadfimportncia colorroyalblue   pltxlabelimportncia pltylabelatributo plttitlefimportncia atributo   nome Pltgcainvertyaxis bar in bar plttext bargetwidth bargety   bargetheight fbargetwidthf vacenter pltshow modelo random forest modelo xgboost tantoparaoxgboostquantoparaorandomforestnotaseagrandesuperioridadeerelevncia atributo redshift classficao estelar pontao superior   outro lado todo outro atrbuto possuir pontuao próximo   possuem baixo relevncia problema   desempenho modelo matriz confuso poder visualizar matriz confuso verificar desempenho previso cada classe    plot matriz confuso cada modelo nome modelo in melhoresmodelositem ypred   modelopredictxtest cm   confusionmatrixytest ypred pltfigurefigsize   snsheatmap cm Annottrue fmtd cmapblue xticklabelsgaláxia quasar estrela yticklabelsgaláxia quasar estrela   pltxlabelpredito pltylabelreal plttitlefmatriz confuso   nome pltshow omodelorandomforestotimizadoapresentaumbomdesempenhonaidentificaodeobjeto galáxia registrar   classificaes correto cerca   amostra classificar quasar   classificar estrela caso quasar   amostra corretamente classificar enquanto nohouveclassificaesincorreta classe estrela entretanto aproximadamente   amostra incorretamente classificar galáxia quanto s estrela   classificaes corretas apenas   amostra ser incorretamente classificar galáxia nenhum amostra classificado erroneamente quasar modelo xgboost otimizar apresentar bom desempenho identificao objeto galáxia registrar   classificaes correto cerca   amostra classificar quasar   classificar estrela caso quasar   amostra corretamente classificar enquanto nohouveclassificaesincorreta classe estrela entretanto aproximadamente   amostra quasar incorretamente classificar galáxia quanto s estrela   classificaes corretas   amostra ser incorretamente classificar galáxia   amostra classificado erroneamente quasar   desempenho modelo roc auc fim utilizaremos último métrico auc técnica onevsrest possibilitar auc médio calcular avaliar todo classe    plot curva roc auc cada modelo nome modelo in melhoresmodelositem ypred   modelopredictxtest yproba   Modelopredictprobaxtest aucmean   rocaucscoreytest yproba multiclassovr averagemacro printfauc médio onevsrest nome aucmeanf in rangeyprobashape fpr tpr    roccurveytest   yproba pltplotfpr tpr labelfclar Pltplot     linestyle colorgray labelaleatório pltxlabeltaxa falso positivo fpr pltylabeltaxa verdadeiro positivo tpr plttitlefcurvar roc multiclar   nome pltlegend pltshow auc médio onevsrest random forest   auc médio onevsrest xgboost   poder ver valor médio obter cada modelo representar excelente capacidade discriminao classe ambos superar   gráfico evidenciar ambos modelo sair bom classificao aleatório configurar classificaes acaso modelo aprender bem mostrar capaz determinar classe alto desempenho   concluso orandom forestotimizadoapresentouomelhordesempenhogeral comumaacuráciade dado teste superar xgboost   entanto tempo treinamento random forest aproximadamente quatro vez grande poder fator limitante cenário onde eficincia computacional crítico atributo redshift mostrouse fundamental classificao ser atributo relevante todo atributo analisar avaliao robusta modelo utilizar múltiplo métrico acurácia preciso revo cao fscorer matriz confuso auc garantir modelo sofrer overfitting apresentar desempenho consistente entanto observamos modelo ainda poder aprimorar especialmente classificao quasar onde pequeno taxa erro futuro melhoria sugerir aplicao técnica pca filtro espectrométrico reduzir dimensionalidade viabilizar otimizaes avanado gridsearch optuna além de isso deploy modelo ambiente produo permitir utilizao tempo real facilitar classificao objeto celeste observaes astronmica'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treated_pdf_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=40, chunk_overlap=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(treated_pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['classificao objeto celeste february', 'february     introduo característica', 'característica espectral so fundamental'] 1769\n"
     ]
    }
   ],
   "source": [
    "print(chunks[:3], len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de embedding (all-MiniLM-L6-v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings[1], len(embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = [f'doc_ {i}' for i in range(len(chunks))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando banco de dados\n",
    "client = chromadb.Client()\n",
    "\n",
    "collection = client.create_collection(name='teste')\n",
    "\n",
    "collection.add(documents=chunks, embeddings=embeddings, ids=uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = model.encode(['Atributos físicos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(query_embeddings=query_embedding, n_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['doc_ 1047', 'doc_ 224', 'doc_ 1023']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['propriedade unidade físico atributo ir',\n",
       "   'anosluz via lácteo    atributo físico',\n",
       "   'físico real havíamos prever grande']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [[None, None, None]],\n",
       " 'distances': [[0.6899502277374268, 0.7843979597091675, 0.8140177726745605]],\n",
       " 'included': [<IncludeEnum.distances: 'distances'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(results['ids'][0])):\n",
    "    doc_id = results['ids'][0][i]\n",
    "    distance = results['distances'][0][i]\n",
    "    document = results['documents'][0][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: doc_ 1023\n",
      "Distância: 0.8140177726745605\n",
      "Documento: físico real havíamos prever grande\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"ID: {doc_id}\")\n",
    "print(f\"Distância: {distance}\")\n",
    "print(f\"Documento: {document}\")\n",
    "print(\"-\" * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
